[{"categories":null,"content":"管用户五 ","date":"2021-12-07","objectID":"/posts/about/:0:0","series":null,"tags":null,"title":"关于我","uri":"/posts/about/#"},{"categories":[],"content":"github action + hugo / vuepress + github pages / gitee pages 自动构建发布 ","date":"2020-11-28","objectID":"/posts/github-action/:0:0","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#"},{"categories":[],"content":"GitHub pages + hugo 新建两个仓库分别用于开发和部署(可用同一仓库) # example # dev repo -\u003e sunnyh1220/docsgo # deploy repo -\u003e sunnyh1220/posts (可选sunnyh1220.github.io 或 sunnyh1220/xxx的gh-pages分支 ) Deploy keys \u0026\u0026 Secrets ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\" # You will get 2 files: # gh-pages.pub (public key) # gh-pages (private key) 部署仓库设置Deploy keys: Settings -\u003e Deploy keys, 添加公钥, Allow write access勾上; 开发仓库设置Secrets: Setting -\u003e Secrets, 添加私钥, name为 ACTIONS_DEPLOY_KEY; 在开发仓库以下路径新建GitHub Actions部署脚本文件: `.github/workflow/hugo-ci.yml name:GitHub Pageson:push:branches:- main # Set a branch to deploypull_request:jobs:deploy:runs-on:ubuntu-20.04concurrency:group:${{ github.workflow }}-${{ github.ref }}steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'0.89.4'extended:true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3if:${{ github.ref == 'refs/heads/main' }}with:deploy_key:${{ secrets.ACTIONS_DEPLOY_KEY }}external_repository:sunnyh1220/posts # github usernamepublish_branch:gh-pagespublish_dir:./public hugo配置 # config.toml # 站点配置 baseURL = \"https://sunnyh1220.github.io/posts\" # 或 https://sunnyh1220.github.io 开发仓库提交后就会自动构建部署. ","date":"2020-11-28","objectID":"/posts/github-action/:0:1","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#github-pages--hugo"},{"categories":[],"content":"GitHub pages + vuepress 仓库配置同上 VuePress配置 // .vuepress/config.js module.exports = { base: '/posts/' // repository name } GitHub Actions脚本文件 `.github/workflow/vuepress-ci.yml # This is a basic workflow to help you get started with Actionsname:Blog CI# Controls when the action will run. on:# Triggers the workflow on push or pull request events but only for the master branchpush:branches:[master ]# Allows you to run this workflow manually from the Actions tabworkflow_dispatch:# A workflow run is made up of one or more jobs that can run sequentially or in paralleljobs:# This workflow contains a single job called \"build\"build:# The type of runner that the job will run onruns-on:ubuntu-latest# Steps represent a sequence of tasks that will be executed as part of the jobsteps:# Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it- uses:actions/checkout@v2# Runs a set of commands using the runners shell- name:Blog buildingrun:|yarn install yarn docs:build - name:Blog Deployuses:peaceiris/actions-gh-pages@v2.5.1env:ACTIONS_DEPLOY_KEY:${{ secrets.ACTIONS_DEPLOY_KEY }}EXTERNAL_REPOSITORY:sunnyh1220/posts# github deploy repository, PUBLISH_BRANCH:gh-pages# deploy branchPUBLISH_DIR:docs/.vuepress/dist ","date":"2020-11-28","objectID":"/posts/github-action/:0:2","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#github-pages--vuepress"},{"categories":[],"content":"gitee pages 参考: https://github.com/yanglbme/gitee-pages-action ","date":"2020-11-28","objectID":"/posts/github-action/:0:3","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#gitee-pages"},{"categories":["Spark"],"content":"Spark ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:0","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#spark"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003efs.defaultFS\u003c/name\u003e \u003cvalue\u003ehdfs://master01:8020\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003ehadoop.tmp.dir\u003c/name\u003e \u003cvalue\u003e/home/sunyh/app/tmp\u003c/value\u003e \u003c/property\u003e ``` hdfs-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003cvalue\u003e1\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003emapreduce.framework.name\u003c/name\u003e \u003cvalue\u003eyarn\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e yarn-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.aux-serivices\u003c/name\u003e \u003cvalue\u003emapreduce_shuffle\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#1-install"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#11-java"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#12-scala"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#13-hadoop"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#14-maven"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#15-spark"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#151-源码编译"},{"categories":["Spark"],"content":"1. install env : centos7 1.1 java 卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scala spark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoop cdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 maven https://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译 编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#152-tar包解压"},{"categories":["Spark"],"content":"2. Spark RDD Resilient Distributed Dataset https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala RDD编程 Parallelized Collections External Datasets PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. RDD Operations transformations actions RDD Persistence ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:2","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#2-spark-rdd"},{"categories":["Spark"],"content":"2. Spark RDD Resilient Distributed Dataset https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala RDD编程 Parallelized Collections External Datasets PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. RDD Operations transformations actions RDD Persistence ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:2","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#rdd编程"},{"categories":["Spark"],"content":"3. PySpark PyCharm环境配置(local 本地测试): python intercepter python structure 添加/home/sunyh/app/spark-2.4.5-bin-hadoop2.6/python/lib下的两个zip包 Run Configurations env 添加PYTHONPATH 测试: from pyspark import SparkConf,SparkContext if __name__ == \"__main__\": # conf = SparkConf().setMaster(\"local[2]\").setAppName(\"spark0526\") conf = SparkConf() sc = SparkContext(conf=conf) data = [1, 2, 3, 4, 5] distData = sc.parallelize(data) print(distData.collect()) sc.stop() 提交pyspark应用程序: 参考:https://spark.apache.org/docs/latest/submitting-applications.html spark-submit --master local[2] --name spark0526 /home/sunyh/py_project/spark_test/test.py spark-submit --help ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:3","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#3-pyspark"},{"categories":["Spark"],"content":"4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \u003cinput\u003e\", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalone https://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarn https://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId \u003capp ID\u003e yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml \u003cproperty\u003e \u003cname\u003eyarn.log-aggregation-enable\u003c/name\u003e \u003cvalue\u003etrue\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds\u003c/name\u003e \u003cvalue\u003e3600\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.remote-app-log-dir\u003c/name\u003e \u003cvalue\u003e/tmp/logs\u003c/value\u003e \u003c/property\u003e 重启yarn,测试 yarn logs --applicationId application_15905667","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#4-spark运行模式"},{"categories":["Spark"],"content":"4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalone https://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarn https://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml yarn.log-aggregation-enable true yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 3600 yarn.nodemanager.remote-app-log-dir /tmp/logs 重启yarn,测试 yarn logs --applicationId application_15905667","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#41-local"},{"categories":["Spark"],"content":"4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalone https://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarn https://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml yarn.log-aggregation-enable true yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 3600 yarn.nodemanager.remote-app-log-dir /tmp/logs 重启yarn,测试 yarn logs --applicationId application_15905667","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#42-standalone"},{"categories":["Spark"],"content":"4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalone https://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarn https://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml yarn.log-aggregation-enable true yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 3600 yarn.nodemanager.remote-app-log-dir /tmp/logs 重启yarn,测试 yarn logs --applicationId application_15905667","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#43-yarn"},{"categories":["Spark"],"content":"5. Monitoring https://spark.apache.org/docs/latest/monitoring.html 配置修改: cd $SPARK_HOME/conf cp spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf # 设置spark.eventLog.enabled和spark.eventLog.dir vim spark-env.sh # 修改SPARK_HISTORY_OPTS # SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=hdfs://master01:8020/spark-logs\" 启动history server: cd $SPARK_HOME/sbin ./start-history-server.sh 不同模式提交任务查看history server: 任务详细信息: 任务日志以json保存在配置的hdfs: ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:5","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#5-monitoring"},{"categories":["Spark"],"content":"6. Spark调优 https://spark.apache.org/docs/latest/tuning.html Data Serialization Memory Tuning Broadcasting Large Variables https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables Data Locality ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:6","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#6-spark调优"},{"categories":["Spark"],"content":"7. Spark SQL https://spark.apache.org/sql/ https://spark.apache.org/docs/latest/sql-programming-guide.html Spark SQL is Apache Spark’s module for working with structured data. Spark SQL提供的操作数据的方式: SQL, DataFrame API, Dataset API(暂时不支持python) Dataset A Dataset is a distributed collection of data. [[Row]]的数据集 DataFrame A DataFrame is a Dataset organized into named columns. 以列(列名,列类型,列值)的的形式构成的分布式数据集. DataFrame = RDD[Row] + shcema 测试文件: spark_sql_op.py from pyspark.sql import SparkSession, Row # Import data types from pyspark.sql.types import * def base_test(spark): # spark is an existing SparkSession df = spark.read.json(\"file:///home/sunyh/app/spark-2.4.5-bin-hadoop2.6/examples/src/main/resources/people.json\") # Displays the content of the DataFrame to stdout df.show() # +----+-------+ # | age| name| # +----+-------+ # |null|Michael| # | 30| Andy| # | 19| Justin| # +----+-------+ # spark, df are from the previous example # Print the schema in a tree format df.printSchema() # root # |-- age: long (nullable = true) # |-- name: string (nullable = true) # Select only the \"name\" column df.select(\"name\").show() # +-------+ # | name| # +-------+ # |Michael| # | Andy| # | Justin| # +-------+ # Select everybody, but increment the age by 1 df.select(df['name'], df['age'] + 1).show() # +-------+---------+ # | name|(age + 1)| # +-------+---------+ # |Michael| null| # | Andy| 31| # | Justin| 20| # +-------+---------+ # Select people older than 21 df.filter(df['age'] \u003e 21).show() # +---+----+ # |age|name| # +---+----+ # | 30|Andy| # +---+----+ # Count people by age df.groupBy(\"age\").count().show() # +----+-----+ # | age|count| # +----+-----+ # | 19| 1| # |null| 1| # | 30| 1| # +----+-----+ def schema_inference_example(spark): \"\"\" RDD --\u003e DataFrame 自动推导Schema :param spark: \"\"\" sc = spark.sparkContext # Load a text file and convert each line to a Row. lines = sc.textFile(\"file:///home/sunyh/app/spark-2.4.5-bin-hadoop2.6/examples/src/main/resources/people.txt\") parts = lines.map(lambda l: l.split(\",\")) people = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) # Infer the schema, and register the DataFrame as a table. schemaPeople = spark.createDataFrame(people) schemaPeople.printSchema() schemaPeople.createOrReplaceTempView(\"people\") # SQL can be run over DataFrames that have been registered as a table. teenagers = spark.sql(\"SELECT name FROM people WHERE age \u003e= 13 AND age \u003c= 19\") # The results of SQL queries are Dataframe objects. # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`. teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect() for name in teenNames: print(name) # Name: Justin def schema_programmatic_example(spark): \"\"\" RDD --\u003e DataFrame 编程实现Schema :param spark: \"\"\" sc = spark.sparkContext # Load a text file and convert each line to a Row. lines = sc.textFile(\"file:///home/sunyh/app/spark-2.4.5-bin-hadoop2.6/examples/src/main/resources/people.txt\") parts = lines.map(lambda l: l.split(\",\")) # Each line is converted to a tuple. people = parts.map(lambda p: (p[0], p[1].strip())) # The schema is encoded in a string. schemaString = \"name age\" fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()] schema = StructType(fields) # Apply the schema to the RDD. schemaPeople = spark.createDataFrame(people, schema) # Creates a temporary view using the DataFrame schemaPeople.createOrReplaceTempView(\"people\") # SQL can be run over DataFrames that have been registered as a table. results = spark.sql(\"SELECT name FROM people\") results.show() # +-------+ # | name| # +-------+ # |Michael| # | Andy| # | Justin| # +-------+ if __name__ == '__main__': spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL basic example\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate() # base_test(spark=spark) # schema_inference_example(spark) schema_programmatic_example(spark) spark.stop() spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/sp","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:7","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#7-spark-sql"},{"categories":["Spark"],"content":"8. Spark Streaming https://spark.apache.org/streaming/ https://spark.apache.org/docs/latest/streaming-programming-guide.html Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. 常用实时流处理框架: Storm: 真正实时流处理 Spark Streaming: 不是真正的实时流处理,而是mini batch操作 Flink Kafka Stream DStream Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. Internally, a DStream is represented as a sequence of RDDs. https://github.com/apache/spark/blob/v2.4.5/examples/src/main/python/streaming/network_wordcount.py nc -lk 9999 cd $SPARK_HOME ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999 ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:8","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#8-spark-streaming"},{"categories":["Spark"],"content":"9. MLlib https://spark.apache.org/mllib/ https://spark.apache.org/docs/latest/ml-guide.html spark.ml –\u003e DataFrame-based API spark.mllib –\u003e RDD-based API ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:9","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#9-mllib"},{"categories":["Spark"],"content":"10. GraphX https://spark.apache.org/graphx/ https://spark.apache.org/docs/latest/graphx-programming-guide.html ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:10","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#10-graphx"},{"categories":["Spark"],"content":"Spark3 参考: https://spark.apache.org/docs/3.0.0-preview2/index.html https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/ https://github.com/rapidsai/spark-examples run a sample Apache Spark Python application that runs on NVIDIA GPUs ","date":"2020-05-25","objectID":"/posts/spark2_basics/:2:0","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#spark3"},{"categories":["NVIDIA","GPU"],"content":" Reference: Hyper-Q Example MULTI-PROCESS SERVICE IMPROVING GPU UTILIZATION WITH MULTI-PROCESS SERVICE (MPS) 多个docker容器如何共享GPU 基于Volta MPS执行资源配置下的多容器共享GPU性能测试 MPS ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:0","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#"},{"categories":["NVIDIA","GPU"],"content":"1. CUDA context 类似于CPU进程上下文,表示与特定进程关联的所有状态 从CPU端分配的GPU上的Global memory (cudaMalloc/cudaMallocManaged) Kernel函数中定义和分配的堆栈空间，例如local memory CUDA streams / events 对象 代码模块(*.cubin, *.ptx) 不同的进程有自己的CUDA context 每个context有自己的地址空间，并且不能访问其他CUDA context的地址空间 时间片轮转调度,相当于串行执行,每个时刻只有一个进程在GPU上执行,吞吐率不会发生变化,而且由于上下文context切换的开销,延迟增加的同时,吞吐量反而是下降的. ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:1","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#1-cuda-context"},{"categories":["NVIDIA","GPU"],"content":"2. Hyper-Q (Hyper Queue) 允许多个CPU 线程或进程同时加载任务到一个GPU上,实现CUDA kernels的并发执行 –- 硬件特性 支持的连接类型 Multi cuda streams Multi cpu threads Multi cpu processes——MPS 管理可并发的最大连接数 # (默认是8) CUDA_DEVICE_MAX_CONNECTIONS = 32 好处 增加GPU利用率（utilization）和占用率（occupancy） 减少CPU空闲时间 增加吞吐率并减少延 使用限制 当kernel A正在执行时, 只有当GPU上任意SM上有足够的资源( 寄存器, 共享内存, 线程块槽位等等)来执行kernel B中的1个线程块时， kernel B才会被发射 要求GPU计算能力大于等于3.5 最大连接数限制： 32 示例代码 $CUDA_PATH/samples/6_Advanced/simpleHyperQ ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:2","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#2-hyper-q-hyper-queue"},{"categories":["NVIDIA","GPU"],"content":"3. MPS - Multi-Process Service，多进程服务 什么是MPS 一组可替换的，二进制兼容的CUDA API实现，包括：守护进程,服务进程,用户运行时 利用GPU上的Hyper-Q 能力 允许多个CPU进程共享同一GPU context 允许不同进程的kernel和memcpy操作在同一GPU上并发执行，以实现最大化 GPU利用率. 好处 提升GPU利用率（时间上）和占用率（空间上） 减少GPU上下文切换时间 减少GPU上下文存储空 使用限制 操作系统: 仅支持linux GPU版本： 大于等于CC 3.5 CUDA版本：大于等于5.5 最大用户连接数量： Pascal及之前GPU: 16 Volta及之后GPU: 48 启动 设置GPU compute mode 为 exclusive mode (非必须，但推荐设置) sudo nvidia-smi -i 0 -c EXCLUSIVE_PROCESS 启动MPS 守护进程 # 指定要启动MPS的GPU,不指定默认对所有GPU生效 export CUDA_VISIBLE_DEVICES=0 # cuda 7.0 以后非必须 export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps # cuda 7.0 以后非必须 export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log nvidia-cuda-mps-control –d 查看MPS 守护进程是否正在运行 ps -ef | grep mps 运行应用程序 mpirun –np 4 ./test 应用程序运行前: 应用程序运行后: 停止 echo quit | nvidia-cuda-mps-control sudo nvidia-smi -i 0 -c 0 监视 nvidia-smi 分析 nvprof --profile-all-processes -o output.%p mpirun –np 4 ./test ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:3","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#3-mps---multi-process-service多进程服务"},{"categories":["AI","分布式训练"],"content":" Reference: https://github.com/horovod/horovod http://www.dataguru.cn/article-14746-1.html Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. ","date":"2020-04-15","objectID":"/posts/horovod_basics/:0:0","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#"},{"categories":["AI","分布式训练"],"content":"1. 分布式深度学习原理 深度学习训练的算法叫做反向传播。即通过神经网络得到预测结果，把预测结果跟标注Label进行比对，发现误差；然后得到神经网络里每个神经元权重导数；接着通过算法得到每个神经元导数，再更新神经元的权重以得到更好的神经元网络，周而复始迭代训练，使得误差减少。 神经网络推理能力随着规模、复杂度增加，能力会极大的增强。但从计算能力角度来说又出现了新问题：很多时候大规模神经网络很难在单个/单点计算单元里面运行，这会导致计算很慢，以至无法运行大规模数据。所以人们提出两种深度学习的基本方法以解决这个问题。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:0","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#1-分布式深度学习原理"},{"categories":["AI","分布式训练"],"content":"1.1 深度学习的两种基本方法 1.1.1 模型并行 把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 1.1.2 数据并行 让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 数据并行原理: ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#11-深度学习的两种基本方法"},{"categories":["AI","分布式训练"],"content":"1.1 深度学习的两种基本方法 1.1.1 模型并行 把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 1.1.2 数据并行 让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 数据并行原理: ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#111-模型并行"},{"categories":["AI","分布式训练"],"content":"1.1 深度学习的两种基本方法 1.1.1 模型并行 把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 1.1.2 数据并行 让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 数据并行原理: ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#112-数据并行"},{"categories":["AI","分布式训练"],"content":"1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#12-实现数据并行的两种工程方法"},{"categories":["AI","分布式训练"],"content":"1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#121-参数服务器-parameter-server"},{"categories":["AI","分布式训练"],"content":"1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#122-ring-allreduce"},{"categories":["AI","分布式训练"],"content":"1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#123-parameter-server与ring-allreduce对比"},{"categories":["AI","分布式训练"],"content":"2. Horovod Horovod是基于Ring-AllReduce方法(参考了百度解决方案)的分布式深度学习插件，以支持多种流行架构包括TensorFlow、Keras、PyTorch等。这样平台开发者只需要为Horovod进行配置，而不是对每个架构有不同的配置方法。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:0","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#2-horovod"},{"categories":["AI","分布式训练"],"content":"2.1 Horovod特点 基于Ring-AllReduce方法 一套插件,支持多种流行架构:Tensorflow, Keras, PyTorch, MxNet 通用环境配置,独立于架构 容易安装与使用 pip install horovod 单机程序稍加修改即可实现分布式学习 与Apache Spark集成 性能高效 支持MPI, NCCL, RDMA, GPUDirect ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#21-horovod特点"},{"categories":["AI","分布式训练"],"content":"2.2 Horovod内部架构 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#22-horovod内部架构"},{"categories":["AI","分布式训练"],"content":"2.4 Horovod使用方法 单机Keras训练脚本: 用Keras定义它的model X-train、Y-train，定义训练样本和测试样本 optimizer，即选定优化器 model.compile model. fit Horovod+Keras分布式训练脚本: 初始化,指定GPU; 定义优化器的时候，优化器重要参数是学习率(lr) , lr * hvd.size(); 使用Horovod的DistributedOptimizer将优化器包裹，抽象了Horovod需要进行的梯度平均的逻辑; 加callback,让Horovod把每次训练的初始状态广播到每个节点，这样保证每个节点从同一个地方开始; epochs平均到每台机器 启动分布式训练: # 单机多卡 horovodrun -np 4 -H localhost:4 python train.py # 多级多卡 horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:3","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#24-horovod使用方法"},{"categories":["AI","分布式训练"],"content":"2.5 高级功能 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:4","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#25-高级功能"},{"categories":["AI","分布式训练"],"content":"2.6 新功能预览 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:5","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#26-新功能预览"},{"categories":["AI","分布式训练"],"content":"2.7 容器中使用Horovod 2.7.1 docker reference: https://github.com/horovod/horovod https://hub.docker.com/r/horovod/horovod/tags 2.7.2 k8s reference: https://github.com/helm/charts/tree/master/stable/horovod https://hub.helm.sh/charts/stable/horovod/1.0.0 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:6","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#27-容器中使用horovod"},{"categories":["AI","分布式训练"],"content":"2.7 容器中使用Horovod 2.7.1 docker reference: https://github.com/horovod/horovod https://hub.docker.com/r/horovod/horovod/tags 2.7.2 k8s reference: https://github.com/helm/charts/tree/master/stable/horovod https://hub.helm.sh/charts/stable/horovod/1.0.0 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:6","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#271-docker"},{"categories":["AI","分布式训练"],"content":"2.7 容器中使用Horovod 2.7.1 docker reference: https://github.com/horovod/horovod https://hub.docker.com/r/horovod/horovod/tags 2.7.2 k8s reference: https://github.com/helm/charts/tree/master/stable/horovod https://hub.helm.sh/charts/stable/horovod/1.0.0 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:6","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#272-k8s"},{"categories":["NVIDIA","GPU"],"content":" 参考: https://docs.nvidia.com/isaac/isaac/doc/overview.html https://developer.nvidia.com/isaac-sdk ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:0:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#"},{"categories":["NVIDIA","GPU"],"content":"1. Isaac Engine application framework Isaac Engine是一个软件框架，可轻松构建模块化机器人应用程序。它为智能机器人提供了高性能的数据处理和深度学习。在Isaac机器人引擎上开发的机器人应用程序可以在NVIDIA®Jetson AGX Xavier™和NVIDIA®Jetson Nano™等边缘设备以及具有离散NVIDIA®GPU的工作站（例如T4）上无缝运行。 ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:1:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#1-isaac-engine"},{"categories":["NVIDIA","GPU"],"content":"2. Isaac GEMs packages with high-performance robotics algorithms GEMs是用于感应，规划或驱动的模块化功能，可以轻松插入机器人应用程序中。例如，开发人员可以添加障碍物检测，立体声深度估计或人类语音识别，以丰富其机器人用例。 ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:2:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#2-isaac-gems"},{"categories":["NVIDIA","GPU"],"content":"3. Isaac Apps reference applications ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:3:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#3-isaac-apps"},{"categories":["NVIDIA","GPU"],"content":"4. Isaac Sim a powerful simulation platform Isaac Sim是虚拟机器人实验室和高保真3D世界模拟器，可通过降低成本和风险来加速研究，设计和开发。 ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:4:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#4-isaac-sim"},{"categories":["NVIDIA","GPU"],"content":"5. Dev ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#5-dev"},{"categories":["NVIDIA","GPU"],"content":"5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u \u003cjetson_username\u003e -h \u003cjetson_ip\u003e engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#51-setup"},{"categories":["NVIDIA","GPU"],"content":"5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 = 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-dependencies-on-the-desktop"},{"categories":["NVIDIA","GPU"],"content":"5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 = 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-bazelhttpsdocsbazelbuildversionsmasterinstall-ubuntuhtml"},{"categories":["NVIDIA","GPU"],"content":"5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 = 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-nvidia-gpu-driver"},{"categories":["NVIDIA","GPU"],"content":"5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 = 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-cuda100httpsdevelopernvidiacomcuda-100-download-archivetarget_oslinuxtarget_archx86_64target_distroubuntutarget_version1804target_typerunfilelocal"},{"categories":["NVIDIA","GPU"],"content":"5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 = 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#install-unity-editor-for-editor-mode"},{"categories":["NVIDIA","GPU"],"content":"5.2 运行Isaac SDK示例App 5.2.1 stereo_dummy sample application cd ~/isaac_sdk bazel build //apps/samples/stereo_dummy bazel run //apps/samples/stereo_dummy 应用运行后,可在浏览器打开http://localhost:3000 将应用部署运行在Jetson上: # 配置desktop免密登录Jetson ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub nona@192.168.20.155 # deploy the package to the robot cd ~/isaac_sdk # ./engine/build/deploy.sh --remote_user \u003cusername_on_robot\u003e -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h \u003crobot_ip\u003e ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 在Jetson上运行 : # ssh ROBOTUSER@ROBOTIP ssh nona@192.168.20.155 # deep为desktop的用户名 cd deploy/deep/stereo_dummy-pkg ./apps/samples/stereo_dummy/stereo_dummy # 应用运行后浏览器打开192.168.20.155:3000 也可以部署到Jetson同时运行App,加 --run ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 --run 5.2.2 v4l2_camera sample application bazel build //apps/samples/v4l2_camera bazel run //apps/samples/v4l2_camera ./engine/build/deploy.sh --remote_user nona -p //apps/samples/v4l2_camera:v4l2_camera-pkg -d jetpack43 -h 192.168.20.155 --run ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:2","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#52-运行isaac-sdk示例app"},{"categories":["NVIDIA","GPU"],"content":"5.2 运行Isaac SDK示例App 5.2.1 stereo_dummy sample application cd ~/isaac_sdk bazel build //apps/samples/stereo_dummy bazel run //apps/samples/stereo_dummy 应用运行后,可在浏览器打开http://localhost:3000 将应用部署运行在Jetson上: # 配置desktop免密登录Jetson ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub nona@192.168.20.155 # deploy the package to the robot cd ~/isaac_sdk # ./engine/build/deploy.sh --remote_user -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 在Jetson上运行 : # ssh ROBOTUSER@ROBOTIP ssh nona@192.168.20.155 # deep为desktop的用户名 cd deploy/deep/stereo_dummy-pkg ./apps/samples/stereo_dummy/stereo_dummy # 应用运行后浏览器打开192.168.20.155:3000 也可以部署到Jetson同时运行App,加 --run ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 --run 5.2.2 v4l2_camera sample application bazel build //apps/samples/v4l2_camera bazel run //apps/samples/v4l2_camera ./engine/build/deploy.sh --remote_user nona -p //apps/samples/v4l2_camera:v4l2_camera-pkg -d jetpack43 -h 192.168.20.155 --run ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:2","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#521--stereo_dummy-sample-application"},{"categories":["NVIDIA","GPU"],"content":"5.2 运行Isaac SDK示例App 5.2.1 stereo_dummy sample application cd ~/isaac_sdk bazel build //apps/samples/stereo_dummy bazel run //apps/samples/stereo_dummy 应用运行后,可在浏览器打开http://localhost:3000 将应用部署运行在Jetson上: # 配置desktop免密登录Jetson ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub nona@192.168.20.155 # deploy the package to the robot cd ~/isaac_sdk # ./engine/build/deploy.sh --remote_user -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 在Jetson上运行 : # ssh ROBOTUSER@ROBOTIP ssh nona@192.168.20.155 # deep为desktop的用户名 cd deploy/deep/stereo_dummy-pkg ./apps/samples/stereo_dummy/stereo_dummy # 应用运行后浏览器打开192.168.20.155:3000 也可以部署到Jetson同时运行App,加 --run ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 --run 5.2.2 v4l2_camera sample application bazel build //apps/samples/v4l2_camera bazel run //apps/samples/v4l2_camera ./engine/build/deploy.sh --remote_user nona -p //apps/samples/v4l2_camera:v4l2_camera-pkg -d jetpack43 -h 192.168.20.155 --run ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:2","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#522--v4l2_camera-sample-application"},{"categories":["NVIDIA","GPU"],"content":"5.3 运行Isaac Sim示例App 5.3.1 Play Mode 5.3.1.1 Warehouse Navigation # start the simulator with the “small_warehouse” scene cd ~/isaac_sim_unity3d/builds ./sample.x86_64 --scene small_warehouse -logFile - # run the Isaac SDK application with the Carter navigation stack cd ~/isaac_sdk bazel run //apps/navsim:navsim_navigate ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:3","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#53-运行isaac-sim示例app"},{"categories":["NVIDIA","GPU"],"content":"5.3 运行Isaac Sim示例App 5.3.1 Play Mode 5.3.1.1 Warehouse Navigation # start the simulator with the “small_warehouse” scene cd ~/isaac_sim_unity3d/builds ./sample.x86_64 --scene small_warehouse -logFile - # run the Isaac SDK application with the Carter navigation stack cd ~/isaac_sdk bazel run //apps/navsim:navsim_navigate ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:3","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#531-play-mode"},{"categories":["NVIDIA","GPU"],"content":"5.3 运行Isaac Sim示例App 5.3.1 Play Mode 5.3.1.1 Warehouse Navigation # start the simulator with the “small_warehouse” scene cd ~/isaac_sim_unity3d/builds ./sample.x86_64 --scene small_warehouse -logFile - # run the Isaac SDK application with the Carter navigation stack cd ~/isaac_sdk bazel run //apps/navsim:navsim_navigate ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:3","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#5311-warehouse-navigation"},{"categories":["CloudNative"],"content":"Kubeadm 安装 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#kubeadm-安装"},{"categories":["CloudNative"],"content":"1. docker install sudo apt-get install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs)stable\" apt-cache madison docker-ce apt-cache madison docker-ce-cli sudo apt-get install docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic containerd.io sudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://registry.docker-cn.com\"] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 开机自启: systemctl enable docker \u0026\u0026 systemctl start docker # systemctl enable kubelet \u0026\u0026 systemctl start kubelet ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:1","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#1-docker-install"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod \u003cpod_name\u003e 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes \u003cnode-name\u003e node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes \u003cnode-name\u003e node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#2-k8s-install-by-kubeadm"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#21-k8s-download"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#22-master-init"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#23-node-init"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#24-cni网络插件"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#25-gpu-插件"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#26-单机"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#27-常用cli"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#设置master节点参与不参与pod负载"},{"categories":["CloudNative"],"content":"2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件 CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#node节点token失效"},{"categories":["CloudNative"],"content":"参考 https://docs.docker.com/storage/storagedriver/ https://docs.docker.com/storage/storagedriver/device-mapper-driver/ Docker修改空间大小 Docker 环境 Storage Pool 用完解决方案：resize-device-mapper 关于docker动态扩展容器空间大小 dmsetup ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:0:1","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#参考"},{"categories":["CloudNative"],"content":"1. Storage Driver storage driver 可以让我们在 container 的可写层中创建数据,当删除 container后,这些数据不会保留;而且可写层的读写速度低于原生文件系统; ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:0","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#1-storage-driver"},{"categories":["CloudNative"],"content":"1.1 images / layers images由一系列的layer组成; images的每一层代表Dockerfile中的一条指令; 对运行中的container所做的所有更改都会写入 Thin R/W layer; ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:1","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#11-images--layers"},{"categories":["CloudNative"],"content":"1.2 container / layers 每个container都有自己的container layer, 并且所有更改都在container layer,所以多个容器可以共享对同一基础image的访问,但有自己的数据状态; Storage Driver 用来管理image layers 和 container layer; docker 提供了多种Storage driver, 每种Storage driver 对实现的处理方式不同,但是都使用可堆叠(stackable)的image layer和写时复制(CoW)策略; ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:2","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#12-container--layers"},{"categories":["CloudNative"],"content":"1.3 container size on disk 查看container size docker ps -s #docker pull node:14.0.0-stretch docker pull node:13.13.0-alpine3.11 #docker run -it -d --name node node:14.0.0-stretch docker run -it -d --name node node:13.13.0-alpine3.11 docker run -it -d --name node_v -v /home/deepbay/deepops/:/deepops node:14.0.0-stretch deep@test3:~$ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 4b7d70e701f1 node:14.0.0-stretch \"docker-entrypoint.s…\" 10 minutes ago Up 10 minutes node 0B (virtual 941MB) deep@test3:~$ docker exec -it node /bin/bash root@4b7d70e701f1:/# wget https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz --2020-04-27 06:18:24-- https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz Resolving dl.google.com (dl.google.com)... 203.208.40.39, 203.208.40.34, 203.208.40.41, ... Connecting to dl.google.com (dl.google.com)|203.208.40.39|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 123658438 (118M) [application/octet-stream] Saving to: 'go1.14.2.linux-amd64.tar.gz' go1.14.2.linux-amd64.tar.gz 100%[====================================================================================\u003e] 117.93M 10.2MB/s in 11s 2020-04-27 06:18:35 (10.7 MB/s) - 'go1.14.2.linux-amd64.tar.gz' saved [123658438/123658438] root@4b7d70e701f1:/# ls -alh | grep go -rw-r--r-- 1 root root 118M Apr 8 22:12 go1.14.2.linux-amd64.tar.gz root@4b7d70e701f1:/# exit exit deep@test3:~$ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 4b7d70e701f1 node:14.0.0-stretch \"docker-entrypoint.s…\" 14 minutes ago Up 14 minutes node 124MB (virtual 1.07GB) 在容器内下载golang安装包,docker container size 增加; size: container layer 的数据量 virtual size: image layers加上 container layer的数据量 ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:3","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#13-container-size-on-disk"},{"categories":["CloudNative"],"content":"1.4 选择 storage driver https://docs.docker.com/storage/storagedriver/select-storage-driver/ overlay2 aufs devicemapper btrfs zfs vfs ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:4","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#14-选择-storage-driver"},{"categories":["CloudNative"],"content":"2. Device Mapper ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:0","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#2-device-mapper"},{"categories":["CloudNative"],"content":"2.1 设置storage driver为device mapper centos 7 sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum list docker-ce --showduplicates | sort -r sudo yum install docker-ce-18.03.1.ce docker-ce-cli-18.03.1.ce containerd.io sudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"storage-driver\": \"devicemapper\" }y EOF sudo systemctl daemon-reload sudo systemctl restart docker # start up automatically on boot up sudo systemctl enable docker \u0026\u0026 sudo systemctl start docker sudo groupadd docker sudo gpasswd -a sunyh docker sudo service docker restart reboot ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:1","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#21-设置storage-driver为device-mapper"},{"categories":["CloudNative"],"content":"2.2 设置 docker Pool Data Space DATA_SIZE=1000 METADATA_SIZE=10 sudo systemctl stop docker dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/data bs=1G count=0 seek=$DATA_SIZE dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/metadata bs=1G count=0 seek=$METADATA_SIZE sudo systemctl start docker ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:2","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#22-设置-docker-pool-data-space"},{"categories":["CloudNative"],"content":"2.3 设置 docker device size (container size) docker pull node:13.13.0-alpine3.11 docker run -it -d --name node node:13.13.0-alpine3.11 docker ps -s dmsetup table resize container echo $((30*1024*1024*1024/512)) echo 0 41943040 thin 253:0 7 | sudo dmsetup load docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a sudo dmsetup resume docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a sudo xfs_growfs -d /dev/mapper/docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 脚本简化以上流程: 获取deviceName # docker inspect -f '{{ .GraphDriver.Data.DeviceName }}' [container id] docker inspect -f '{{ .GraphDriver.Data.DeviceName }}' 2d9ddb14b348 使用dmsetup table 命令显示出device mapper的具体信息 # dmsetup table [device name] sudo dmsetup table docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 当容器在stop状态时以上操作得不到device mapper信息，虽然由于空 间不足不能启动成功，但需要先docker start 才能查找到对应 device mapper信息。 算出想要扩容的扇区数 SIZE=15 echo $(($SIZE*1024*1024*1024/512)) load一个新的设备信息表(新表与旧表相比只修改扇区数量) echo 0 31457280 thin 253:0 7 | sudo dmsetup load docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 通过dmsetup resume激活新的设备信息表 sudo dmsetup resume docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 调整文件系统大小(只能增,不能减) File Type Resize CLI xfs xfs_growfs ex2,ex3,ex4 resize2fs sudo xfs_growfs -d /dev/mapper/docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a import paramiko hostname = '192.168.232.133' username = 'sunyh' password = 'hisunyh' container_id = \"3afd866418592636fc27754bfebdf29576daa5c22118e323a912320571dcabc0\" device_size = 16 paramiko.util.log_to_file('syslogin.log') # 发送paramiko日志到syslogin.log文件 def resize_container(): ssh = paramiko.SSHClient() # 获取客户端host_keys,默认~/.ssh/known_hosts,非默认路径需指定ssh.load_system_host_keys(/xxx/xxx) ssh.load_system_host_keys() ssh.connect(hostname=hostname, username=username, password=password) # echo \"yourpasswd\" |sudo -S yourcommand stdin, stdout, stderr = ssh.exec_command(\"docker inspect -f '{{ .GraphDriver.Data.DeviceName }}' %s\" % container_id) # print(stdout.read().decode('utf-8')) device_name = stdout.readlines()[0] if device_name.find('docker') == -1: print('get device name failed') return print('device name: %s' % device_name) stdin, stdout, stderr = ssh.exec_command(\"echo %s| sudo -S dmsetup table %s\" % (password, device_name)) device_mapper = stdout.readlines()[0] if device_mapper.find('thin') == -1: print('get device mapper info failed') return print('device mapper info: %s' % device_mapper) device_sector = device_mapper.replace('\\n', '').split('thin')[1] print('device sector: %s' % device_sector) stdin, stdout, stderr = ssh.exec_command('echo 0 %sthin%s| sudo dmsetup load %s' % (device_size * 1024 * 1024 * 1024 / 512, device_sector, device_name)) print('dmsetup load finish') stdin, stdout, stderr = ssh.exec_command('sudo dmsetup resume %s' % device_name) print('dmsetup resume finish') fs_line = 'sudo xfs_growfs -d /dev/mapper/%s' % device_name print(fs_line) stdin, stdout, stderr = ssh.exec_command(fs_line) print(stdout.readlines()) print('file system modify finish') ssh.close() pass if __name__ == \"__main__\": resize_container() ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:3","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#23-设置-docker-device-size-container-size"},{"categories":["CloudNative"],"content":"下载安装 Environment: Ubuntu18.04 Reference: https://docs.docker.com/engine/install/ubuntu/ # 卸载 docker sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs)stable\" # docker version list apt-cache madison docker-ce apt-cache madison docker-ce-cli # install sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # 安装指定版本 sudo apt-get install docker-ce=5:18.09.9~3-0~ubuntu-bionic docker-ce-cli=5:18.09.9~3-0~ubuntu-bionic containerd.io # 设置 daemon.json sudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://registry.docker-cn.com\"] } EOF # reboot docker sudo systemctl daemon-reload sudo systemctl restart docker # start up automatically on boot up sudo systemctl enable docker \u0026\u0026 sudo systemctl start docker # docker permission deny sudo groupadd docker sudo gpasswd -a [user name] docker sudo service docker restart ","date":"2019-11-20","objectID":"/posts/docker_basics/:1:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#下载安装"},{"categories":["CloudNative"],"content":"修改存储位置 // 修改docker默认镜像/容器存储位置 // Docker默认的镜像和容器存储位置在/var/lib/docke 1)修改docker.service文件　cd /etc/systemd/system/multi-user.target.wants sudo vim docker.service // ExecStart=/usr/bin/dockerd --graph=/data/docker --storage-driver=overlay --graph=/data/docker：docker新的存储位置 --storage-driver=overlay ： 当前docker所使用的存储驱动 2) 重启docker　sudo systemctl daemon-reload sudo systemctl restart docker ","date":"2019-11-20","objectID":"/posts/docker_basics/:2:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#修改存储位置"},{"categories":["CloudNative"],"content":"Docker常用操作 # docker --help build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes # docker build --help Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Run a command in a new container Options: --add-host list Add a custom host-to-IP mapping (host:ip) -d, --detach Run container in background and print container ID -e, --env list Set environment variables --gpus gpu-request GPU devices to add to the container ('all' to pass all GPUs) --help Print usage -h, --hostname string Container host name -i, --interactive Keep STDIN open even if not attached -m, --memory bytes Memory limit --mount mount Attach a filesystem mount to the container --name string Assign a name to the container --network network Connect a container to a network --privileged Give extended privileges to this container -p, --publish list Publish a container's port(s) to the host --rm Automatically remove the container when it exits --shm-size bytes Size of /dev/shm -t, --tty Allocate a pseudo-TTY --ulimit ulimit Ulimit options (default []) -u, --user string Username or UID (format: \u003cname|uid\u003e[:\u003cgroup|gid\u003e]) -v, --volume list Bind mount a volume -w, --workdir string Working directory inside the container Examples # list all containers docker container ls -a / docker ps -a # list all containers id docker container ls -aq / docker contaier ls -a | awk {'print$1'} # remove all containers docker rm $(docker container ls -aq) # list all exited containers docker container ls -f \"status=exited\" # list all exited containers id docker container ls -f \"status=exited\" -q docker ps -a|grep Exited|awk '{print $1}' # remove all exited containers docker rm $(docker container ls -f \"status=exited\" -q) docker rm `docker ps -a|grep Exited|awk '{print $1}'` docker rm $(docker ps -qf status=exited) #删除名称或标签为none的镜像 docker rmi -f `docker images | grep '\u003cnone\u003e' | awk '{print $3}'` #删除所有名字中带 “dee” 关键字的镜像 docker rmi $(docker images | grep \"dee\" | awk '{print $3}') # docker commit [CONTAINER] [REPOSITORY] docker run -d -it --name nginx nginx docker commit nginx hisunyh/my-nginx:1.0 docker run -d -it --name crawler --add-host master.namenode:192.168.1.25 --add-host slave.datanode1:192.168.1.26 --add-host slave.datanode2:192.168.1.29 -e \"queu","date":"2019-11-20","objectID":"/posts/docker_basics/:3:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#docker常用操作"},{"categories":["CloudNative"],"content":"Dockerfile Refenence: https://docs.docker.com/engine/reference/builder/ https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ Samples sudo tee ./Dockerfile.dev \u003c\u003c-'EOF' FROM golang:1.14-alpine AS prod-build ENV GOPROXY https://goproxy.cn,direct # ENV GO111MODULE on WORKDIR /workspace COPY . /workspace RUN CGO_ENABLED=0 GOOS=linux go build -o deep_arena . # FROM scratch AS prod-final FROM debian:9.12 AS prod-final RUN rm -f /etc/localtime \\ \u0026\u0026 ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ \u0026\u0026 echo \"Asia/Shanghai\" \u003e /etc/timezone COPY --from=prod-build /workspace/deep_arena . COPY --from=prod-build /workspace/conf.toml . COPY --from=prod-build /workspace/config . EXPOSE 8082 CMD [\"/deep_arena\"] EOF docker login my-horbor.com docker build -f ./Dockerfile.dev -t my-horbor.com/deep_arena:dev_v1.0 . docker push my-horbor.com/deep_arena:dev_v1.0 docker pull my-horbor.com/deep_arena:dev_v1.0 docker run -d -it -p 9082:8082 --name da -v ~/pod_logs:/pod_logs my-horbor.com/deep_arena:dev_v1.0 ","date":"2019-11-20","objectID":"/posts/docker_basics/:4:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#dockerfile"},{"categories":["CloudNative"],"content":"Kafka, MongoDB ","date":"2019-11-20","objectID":"/posts/docker_examples/:0:0","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#"},{"categories":["CloudNative"],"content":"Kafka docker pull wurstmeister/zookeeper docker pull wurstmeister/kafka docker run -d --name zookeeper -p 2181:2181 -t wurstmeister/zookeeper docker run -d --name kafka --publish 9092:9092 \\ --link zookeeper \\ --env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\ --env KAFKA_ADVERTISED_HOST_NAME=127.0.0.1 \\ --env KAFKA_ADVERTISED_PORT=9092 \\ wurstmeister/kafka test # smoke test docker exec -it kafka /bin/bash $ /opt/kafka/bin/kafka-console-producer.sh --topic=test --broker-list localhost:9092 $ /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 -from-beginning --topic test ","date":"2019-11-20","objectID":"/posts/docker_examples/:1:0","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#kafka"},{"categories":["CloudNative"],"content":"MongoDB ","date":"2019-11-20","objectID":"/posts/docker_examples/:2:0","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#mongodb"},{"categories":["CloudNative"],"content":"1. install mongodb by docker 转自 https://blog.csdn.net/weixin_44591832/article/details/91953189 # docker pull mongo:4.2.5 docker pull mongo:3.6.17 # docker run --name mongodb -p 27017:27017 -d mongo:4.2.5 --auth docker run --name mongodb -p 27017:27017 -d mongo:3.6.17 MongoDB添加管理员 docker exec -it mongodb mongo admin 创建admini管理员账号: # 在所有数据库管理用户 db.createUser({ user: 'root', pwd: 'root', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] }); # 授权管理所有数据库 db.createUser({ user: 'admin', pwd: 'admin123', roles: [ { role: \"dbAdminAnyDatabase\", db: \"admin\" } ] }); $ docker exec -it mongodb mongo admin MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/admin?compressors=disabled\u0026gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"685f78a0-4730-4192-a5c5-39677312e765\") } MongoDB server version: 4.2.5 Welcome to the MongoDB shell. For interactive help, type \"help\". For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user \u003e db.createUser({ user: 'root', pwd: 'root', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] }); Successfully added user: { \"user\" : \"root\", \"roles\" : [ { \"role\" : \"userAdminAnyDatabase\", \"db\" : \"admin\" } ] } \u003e exit bye 创建普通用户: docker exec -it mongodb mongo admin db.auth(\"root\",\"root\"); db.createUser({ user: 'sunyh', pwd: 'hi123456', roles: [ { role: \"readWrite\", db: \"ngc\" } ] }); db.createUser({ user: 'sunyh', pwd: 'hi123456', roles: [ { role: \"dbOwner\", db: \"ngc\" } ] }); $ docker exec -it mongodb mongo admin MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/admin?compressors=disabled\u0026gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"6658ca1e-15e5-40e4-a507-8a9fab70bee1\") } MongoDB server version: 4.2.5 \u003e db.auth(\"root\",\"root\"); 1 \u003e db.createUser({ user: 'sunyh', pwd: 'hi123456', roles: [ { role: \"dbOwner\", db: \"ngc\" } ] }); Successfully added user: { \"user\" : \"sunyh\", \"roles\" : [ { \"role\" : \"dbOwner\", \"db\" : \"app\" } ] } \u003e exit bye db.auth(\"sunyh\",\"hi123456\"); use ngc db.test.save({name:\"hello ngc\"}); $ docker exec -it mongodb mongo admin MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/admin?compressors=disabled\u0026gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"19c22492-6b5b-4a99-bbcc-a2c852d003cb\") } MongoDB server version: 4.2.5 \u003e db.auth(\"sunyh\",\"hi123456\"); 1 \u003e use ngc switched to db ngc \u003e db.test.save({name:\"hello ngc\"}); WriteResult({ \"nInserted\" : 1 }) \u003e exit bye ","date":"2019-11-20","objectID":"/posts/docker_examples/:2:1","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#1-install-mongodb-by-docker"},{"categories":["CloudNative"],"content":"2. mongo express https://github.com/mongo-express/mongo-express-docker docker pull mongo-express:0.54 docker run -it -d --name mongo-express --link mongodb:mongo -p 28081:8081 mongo-express:0.54 docker run -it --rm \\ --name mongo-express \\ --link mongodb:mongo \\ -p 8081:8081 \\ -e ME_CONFIG_OPTIONS_EDITORTHEME=\"ambiance\" \\ -e ME_CONFIG_BASICAUTH_USERNAME=\"user\" \\ -e ME_CONFIG_BASICAUTH_PASSWORD=\"fairly long password\" \\ mongo-express docker stop mongo-express docker rm mongo-express ","date":"2019-11-20","objectID":"/posts/docker_examples/:2:2","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#2-mongo-express"},{"categories":["FS"],"content":" curl -k -u admin:admin001 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"fset007\", \"path\": \"/gpfs/fset007\", \"owner\": \"sunyh\", \"permissions\": \"700\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' curl -k -u admin:admin001 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"fset007\", \"path\": \"/gpfs/fset007\", \"owner\": \"sunyh\", \"permissions\": \"700\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' curl -k -u admin:admin001 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"fset007\", \"path\": \"/gpfs/fset007\", \"owner\": \"sunyh\", \"permissions\": \"700\", \"inodeSpace\": \"root\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' curl -k -u admin:admin001 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/jobs/1000000000013' ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:0","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#"},{"categories":["FS"],"content":"创建文件集 curl -k -u admin001:deepbay2010 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"feitest\", \"owner\" : \"root\", \"path\": \"/gpfs/feitest\", \"permissions\": \"777\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' 注释： filesetName ：文件集名称 owner： 新文件集所有者的用户ID，然后可选地加上组ID，例如“ root：”。 path：文件集路径 permissions：权限， 相当于使用chmod设置的权限 tip：创建文件集的时候，path不能存在，否则会创建失败。 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:1","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#创建文件集"},{"categories":["FS"],"content":"设置文件集配额 curl -k -u admin001:deepbay2010 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{\"operationType\": \"setQuota\",\"quotaType\": \"FILESET\",\"objectName\": \"feitest\",\"blockSoftLimit\": \"1G\",\"blockHardLimit\": \"1G\"}' \"https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/quotas\" 注释： objectName：文件集名称 blockSoftLimit：软限制 （警告） blockHardLimit：硬限制 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:2","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#设置文件集配额"},{"categories":["FS"],"content":"删除文件集 curl -k -u admin001:deepbay2010 -X DELETE --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets/{feilSetName}' 注释： 路径最后为文件集名称 tip：删除文件集会把该文件集对应路径下的所有文件删除 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:3","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#删除文件集"},{"categories":["FS"],"content":"获取指定文件集信息 curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets/{feilSetName}' ​ ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:4","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取指定文件集信息"},{"categories":["FS"],"content":"获取简略的所有文件集信息 curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:5","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取简略的所有文件集信息"},{"categories":["FS"],"content":"获取指定文件集配额信息 curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets/{feilSetName}/quotas' ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:6","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取指定文件集配额信息"},{"categories":["FS"],"content":"获取配额使用信息 (所有的文件集) curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/quotas' tip：等效于 mmrepquota -g -v -a ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:7","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取配额使用信息----所有的文件集"},{"categories":["FS"],"content":"注意事项 上传文件超过配额大小时，会中止上传，但是已上传的部分会留在磁盘内，建议删除已上传的部分。 不能使用root用户进行文件上传，否则上传配额失效。 gui页面 https://192.168.1.50 官网 https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_apiv2postfilesystemfilesets.htm 版本：4.2.3 ，V2 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:8","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#注意事项"},{"categories":["FS"],"content":"GPFS安装配置 ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#gpfs安装配置"},{"categories":["FS"],"content":"1. 操作系统配置 SELINUX 修改/etc/selinux/config 文件将 selinux 设置为 disable。 防火墙 使用 systemctl stop firewalld 来关闭防火墙，使用 systemctl disable firewalld 来 禁用防火墙自动启动服务。 软件包 将软件安装所需的依赖软件包件复制到了/root/rhel72/目录中 # scp -r rhel72 root@192.168.1.48:/root 网络配置 更新服务器/etc/hosts # gpfs 192.168.1.50 gpfs-io-1 192.168.1.48 gpfs-io-2 192.168.1.162 gpfs-cli-1 192.168.1.33 gpfs-cli-2 192.168.1.29 gpfs-cli-3 192.168.1.30 gpfs-cli-4 ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:1","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#1-操作系统配置"},{"categories":["FS"],"content":"2. I/O 服务器端安装 准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 # 每个节点执行 # 免密 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-2 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-2 2.1 软件安装 安装 gpfs rpm 包 # 基础包 # -ivh cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -ivh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # 安装升级包 # -Uvh # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # --force # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh --force gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # -------------------- # ----- Ubuntu ----- # cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb # 卸载 # dpkg -l | grep gpfs # dpkg -P gpfs.base gpfs.docs gpfs.ext gpfs.gpl gpfs.gskit gpfs.gss.pmcollector gpfs.gss.pmsensors gpfs.java gpfs.msg.en-us 编译 cd /usr/lpp/mmfs/src \u0026\u0026 make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX \u0026\u0026 make World \u0026\u0026 make InstallImages 如果编译提示没有kernel-headers,安装对应的版本的kernel-headers # yum install -y kernel-headers kernel-devel cd ~/rhel72/centos7.7-kernel/ \u0026\u0026 rpm -ivh kernel-headers-3.10.0-1062.el7.x86_64.rpm kernel-devel-3.10.0-1062.el7.x86_64.rpm yum install -y gcc gcc-c++ 将gpfs执行文件路径添加到环境变量 echo 'PATH=$PATH:/usr/lpp/mmfs/bin' \u003e\u003e /etc/profile \u0026\u0026 source /etc/profile 删除已有集群节点: # https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1pdg_nodnoad.htm mmdelnode -f 2.2 初始化配置文件 创建存储集群 只需在 gpfs-io-1 配置 创建存储集群需要两个配置文件，一个用于设定服务器的角色(mmcrcluster-node.lst) ， 一 个 用 于 设 定 存 储 集 群 的 参 数 配 置(mmcrcluster-config.lst)，两个文件的内容参考如下： tee ~/mmcrcluster-node.lst \u003c\u003c-'EOF' gpfs-io-1:manager-quorum gpfs-io-2:manager-quorum EOF tee ~/mmcrcluster-config.lst \u003c\u003c-'EOF' pagepool 4096M maxMBpS 22400 maxblocksize 16m EOF 创建集群: mmcrcluster -N mmcrcluster-node.lst -p gpfs-io-1 -s gpfs-io-2 -r /usr/bin/ssh -R /usr/bin/scp -C GridScaler-Cluster -A -c mmcrcluster-config.lst 为节点添加License: mmchlicense server --accept -N gpfs-io-1,gpfs-io-2 .创建 NSD 磁盘 Multipath配置 # http://www.linuxboy.net/linuxjc/144375.html yum install -y device-mapper-multipath mpathconf --enable lsmod |grep dm_multipath modprobe dm-multipath modprobe dm-round-robin service multipathd start multipath –v2 multipath -ll 如果multipath -ll没输出,编辑配置文件/etc/multipath.conf,将find_multipaths yes 注释 vi /etc/multipath.conf 重启,然后验证是否配置成功 准备并行文件系统的磁盘检测脚本 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices编辑/var/mmfs/etc/nsddevices 文件，如下图所示增加/dev/mapper 一行内容。 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices \u0026\u0026 chmod +777 /var/mmfs/etc/nsddevices \u0026\u0026 vi /var/mmfs/etc/nsddevices # ls -l /dev/mapper/ 2\u003e/dev/null | awk '{print \"mapper/\"$9 \" generic\"}' 重启每一台服务器 init 6 编写 NSD 磁盘脚本 tee ~/nsd.conf \u003c\u003c-'EOF' %nsd: nsd=NSD_1 device=/dev/mapper/mpathb servers=gpfs-io-1,gpfs-io-2 usage=dataAndMetadata failureGroup=100 pool=system EOF servers 为该 LUN 被访问的 I/O 服务器顺序，当 gpfs-io-1 服务器关机或宕机时，gpfs-io-2 优先取得该 LUN 的控制权. 创建 NSD 磁盘 # 创建 NSD 磁盘 mmcrnsd -F nsd.conf # 删除 NSD 磁盘 # mmdelnsd -F nsd.conf 创建磁盘过程出现错误: refers to an existing NSD, 加上 -v no: 参考:https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_mmcrnsd.htm mmcrnsd -F -v no nsd.conf 创建文件系统 # 创建文件系统 mmcrfs gpfs -F nsd.conf -m 1 -M 2 -r 1 -R 2 -A yes -B 16m -n 512 -j cluster -Q yes -T /gpfs # 查看文件系统 mmlsfs all 上述命令中 gpfs 为文件系统名称，-A yes 代表文件系统自动 mount，-B 16m 代表文件系统块大小为 16m，-n 512 代表该集群最大支持节点数，-","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#2-io-服务器端安装"},{"categories":["FS"],"content":"2. I/O 服务器端安装 准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 # 每个节点执行 # 免密 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-2 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-2 2.1 软件安装 安装 gpfs rpm 包 # 基础包 # -ivh cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -ivh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # 安装升级包 # -Uvh # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # --force # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh --force gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # -------------------- # ----- Ubuntu ----- # cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb # 卸载 # dpkg -l | grep gpfs # dpkg -P gpfs.base gpfs.docs gpfs.ext gpfs.gpl gpfs.gskit gpfs.gss.pmcollector gpfs.gss.pmsensors gpfs.java gpfs.msg.en-us 编译 cd /usr/lpp/mmfs/src \u0026\u0026 make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX \u0026\u0026 make World \u0026\u0026 make InstallImages 如果编译提示没有kernel-headers,安装对应的版本的kernel-headers # yum install -y kernel-headers kernel-devel cd ~/rhel72/centos7.7-kernel/ \u0026\u0026 rpm -ivh kernel-headers-3.10.0-1062.el7.x86_64.rpm kernel-devel-3.10.0-1062.el7.x86_64.rpm yum install -y gcc gcc-c++ 将gpfs执行文件路径添加到环境变量 echo 'PATH=$PATH:/usr/lpp/mmfs/bin' /etc/profile \u0026\u0026 source /etc/profile 删除已有集群节点: # https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1pdg_nodnoad.htm mmdelnode -f 2.2 初始化配置文件 创建存储集群 只需在 gpfs-io-1 配置 创建存储集群需要两个配置文件，一个用于设定服务器的角色(mmcrcluster-node.lst) ， 一 个 用 于 设 定 存 储 集 群 的 参 数 配 置(mmcrcluster-config.lst)，两个文件的内容参考如下： tee ~/mmcrcluster-node.lst /dev/null | awk '{print \"mapper/\"$9 \" generic\"}' 重启每一台服务器 init 6 编写 NSD 磁盘脚本 tee ~/nsd.conf ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#21-软件安装"},{"categories":["FS"],"content":"2. I/O 服务器端安装 准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 # 每个节点执行 # 免密 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-2 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-2 2.1 软件安装 安装 gpfs rpm 包 # 基础包 # -ivh cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -ivh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # 安装升级包 # -Uvh # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # --force # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh --force gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # -------------------- # ----- Ubuntu ----- # cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb # 卸载 # dpkg -l | grep gpfs # dpkg -P gpfs.base gpfs.docs gpfs.ext gpfs.gpl gpfs.gskit gpfs.gss.pmcollector gpfs.gss.pmsensors gpfs.java gpfs.msg.en-us 编译 cd /usr/lpp/mmfs/src \u0026\u0026 make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX \u0026\u0026 make World \u0026\u0026 make InstallImages 如果编译提示没有kernel-headers,安装对应的版本的kernel-headers # yum install -y kernel-headers kernel-devel cd ~/rhel72/centos7.7-kernel/ \u0026\u0026 rpm -ivh kernel-headers-3.10.0-1062.el7.x86_64.rpm kernel-devel-3.10.0-1062.el7.x86_64.rpm yum install -y gcc gcc-c++ 将gpfs执行文件路径添加到环境变量 echo 'PATH=$PATH:/usr/lpp/mmfs/bin' /etc/profile \u0026\u0026 source /etc/profile 删除已有集群节点: # https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1pdg_nodnoad.htm mmdelnode -f 2.2 初始化配置文件 创建存储集群 只需在 gpfs-io-1 配置 创建存储集群需要两个配置文件，一个用于设定服务器的角色(mmcrcluster-node.lst) ， 一 个 用 于 设 定 存 储 集 群 的 参 数 配 置(mmcrcluster-config.lst)，两个文件的内容参考如下： tee ~/mmcrcluster-node.lst /dev/null | awk '{print \"mapper/\"$9 \" generic\"}' 重启每一台服务器 init 6 编写 NSD 磁盘脚本 tee ~/nsd.conf ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#22-初始化配置文件"},{"categories":["FS"],"content":"3. Linux 计算节点客户端安装 准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 # io node ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22 root@192.168.1.29 ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22 root@192.168.1.30 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 关闭计算节点的防火墙。 设置网络 软件安装: # centos cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # ubuntu cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb 编译: cd /usr/lpp/mmfs/src make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX make World make InstallImages 客户端加到gpfs: mmaddnode -N gpfs-cli-1 mmaddnode -N gpfs-cli-2 mmaddnode -N gpfs-cli-3 mmaddnode -N gpfs-cli-4 # 删除节点 # mmdelnode -N gpfs-cli-1 # 删除全部节点 # mmdelnode -a # 节点与cluster失联,可以在node节点删除 # mmdelnode -f # if addnode error sudo apt-get autoremove sudo apt --fix-broken install 客户端添加License: mmchlicense client --accept -N gpfs-cli-1 mmchlicense client --accept -N gpfs-cli-2 mmchlicense client --accept -N gpfs-cli-3 mmchlicense client --accept -N gpfs-cli-4 ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:3","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#3-linux-计算节点客户端安装"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#gpfs-常用cli"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统服务启动"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统服务的停止"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统服务状态"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#手工-mountumount-文件系统"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看文件系统-mount-状态"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统集群信息"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统集群配置信息"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统-gpfs-的配置信息"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统集群的磁盘信息"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统对应的磁盘信息"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统重要配置文件"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统的日志文件"},{"categories":["FS"],"content":"GPFS 常用CLI 并行文件系统服务启动 mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止 mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态 mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！ 通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态 mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息 mmlscluster 查看并行文件系统集群配置信息 mmlsconfig 查看并行文件系统 gpfs 的配置信息 mmlsfs gpfs 查看并行文件系统集群的磁盘信息 mmlsnsd -M 查看并行文件系统对应的磁盘信息 mmlsdisk gpfs -M 并行文件系统重要配置文件 /var/mmfs/gen/mmsdrfs 并行文件系统的日志文件 当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额 首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#设置磁盘配额"},{"categories":["FS"],"content":"GPFS Quota Reference: Managing GPFS quotas Filesets GPFS fileset level quota management in pureScale environment ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:3:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#gpfs-quota"},{"categories":["FS"],"content":"CLI # 文件集相关信息 mmlsfileset gpfs -L # 启用配额 mmchfs gpfs -Q yes --perfileset-quota # 启用默认配额 mmdefquotaon # 为新用户，组和文件集指定默认配额值 mmdefedquota # 设置默认配额 mmsetquota –F /tmp/defaultQuotaExample # 显式建立或更改文件系统配额限制 #https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_mmedquota.htm mmedquota # 检查配额 mmcheckquota mmcrfileset mmdelfileset mmlinkfileset mmunlinkfileset mmlsfileset gpfs mmdefquotaon -d gpfs mmcrfileset gpfs fset1 -t \"test file set\" mmedquota -j gpfs:fset1 mmcheckquota -v gpfs mmrepquota -g -v -a # mmedquota -t -j # 链接文件集与文件夹 mmlinkfileset gpfs fset1 -J /gpfs/fset1 mmunlinkfileset gpfs fset1 mmcrfileset gpfs fset2 -t \"test file set\" mmedquota -u sunyh mmedquota -u gpfs:fset2:sunyh mmrepquota -v -a mmrepquota -j -v -a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:3:1","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#cli"},{"categories":["FS"],"content":"REST API Scale management API v2 install \u0026\u0026 manager user https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_configapi.htm#bl1adm_configapi # 默认拥有所有权限的用户名密码 admin:admin001 # 修改admin密码 /usr/lpp/mmfs/gui/cli/chuser admin -p newPassword # 新建管理user usr/lpp/mmfs/gui/cli/mkuser yum install gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm yum install gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm yum install gpfs.java-4.2.3-22.x86_64.rpm yum install gpfs.gui-4.2.3-22.noarch.rpm # yum reinstall gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm # yum reinstall gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm # yum reinstall gpfs.java-4.2.3-22.x86_64.rpm # yum reinstall gpfs.gui-4.2.3-22.noarch.rpm systemctl start gpfsgui systemctl enable gpfsgui cd /usr/lpp/mmfs/gui/cli/ ./mkuser admin -g SecurityAdmin systemctl start gpfsgui systemctl status gpfsgui.service ./mkuser admin -g SecurityAdmin ./mkuser admin001 -g SecurityAdmin curl -k -u admin:admin001 -XGET -H content-type:application/json \"https://192.168.1.50:443/scalemgmt/v2/info\" test curl -k -u admin001:deepbay2010 -XGET -H content-type:application/json \"https://192.168.1.50:443/scalemgmt/v2/info\" Enabling performance tools in management GUI mmperfmon config generate --collectors=gpfs-io-1,gpfs-io-2 mmchnode --perfmon -N gpfs-io-1,gpfs-io-2,gpfs-cli-1,gpfs-cli-2 mmperfmon config update GPFSDiskCap.restrict=gpfs-io-1,gpfs-io-2 GPFSDiskCap.period=86400 mmperfmon config update GPFSFilesetQuota.restrict=gpfs-io-1,gpfs-io-2 GPFSFilesetQuota.period=3600 curl -k -u admin:admin001 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets?fields=:all:' curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems' curl -k -u admin:admin001 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/config?fields=:all:' scp gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm root@192.168.1.52:/root/rhel72/4.2.3.22/gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm scp gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm root@192.168.1.52:/root/rhel72/4.2.3.22/gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm curl -k -u admin-storage:123456 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems' ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:3:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#rest-api"},{"categories":["FS"],"content":"TroubleShooting kernel extension does not exist reference: https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1ins_bldgpl.htm#bldgpl /usr/lpp/mmfs/bin/mmbuildgpl --build-package # Wrote: /tmp/deb/gpfs.gplbin-4.15.0-118-generic_4.2.3-22_amd64.deb dpkg -i /tmp/deb/gpfs.gplbin-4.15.0-118-generic_4.2.3-22_amd64.deb ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:4:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#troubleshooting"},{"categories":["node"],"content":"nvm,npm,nrm ","date":"2019-11-20","objectID":"/posts/node_install/:0:0","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#"},{"categories":["node"],"content":"nvm nvm git curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bash source .bashrc nvm --version nvm ls-remote nvm install 10.16.0 which node nvm list nvm use 10 nvm alias default 10 nvm reinstall-packages 10 ","date":"2019-11-20","objectID":"/posts/node_install/:0:1","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#nvm"},{"categories":["node"],"content":"npm npm -v npm install -g npm // -P -D -g ","date":"2019-11-20","objectID":"/posts/node_install/:0:2","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#npm"},{"categories":["node"],"content":"nrm npm install --global nrm nrm test nrm ls nrm use cnpm // 推荐使用cnpm部署私有源镜像服务器 nrm add yourcompany http://registry.npm.yourcompany.com/ ","date":"2019-11-20","objectID":"/posts/node_install/:0:3","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#nrm"},{"categories":["node"],"content":"源码编译node sudo apt-get install g++ curl libssl-dev apache2-utils git-core build-essential git clone https://github.com/nodejs/node.git cd node ./configure make sudo make install ","date":"2019-11-20","objectID":"/posts/node_install/:0:4","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#源码编译node"},{"categories":["DB"],"content":"redis redis-server stop redis-server start redis-server restart src/redis-server redis.conf bin/mongod -f mongodb.conf ","date":"2019-11-20","objectID":"/posts/redis-cli/:1:0","series":null,"tags":["redis"],"title":"redis cli","uri":"/posts/redis-cli/#redis"},{"categories":["DB"],"content":"redis-cli redis-cli -h \u003chost\u003e -p \u003cport\u003e select 1 //checkout db1, normal db0 keys * type \u003ckey_name\u003e flushdb flushall list LPUSH mylist \"helllo\" LRANGE mylist 0 -1 LLEN mylist set SADD myset \"hello\" SMEMBERS myset SCARD myset zset ZADD myzset 1 \"one\" 2 \"two\" ZRANGE myzset 0 -1 WITHSCORES ZCARD myzset ","date":"2019-11-20","objectID":"/posts/redis-cli/:1:1","series":null,"tags":["redis"],"title":"redis cli","uri":"/posts/redis-cli/#redis-cli"},{"categories":["DB"],"content":"python redis https://github.com/andymccurdy/redis-py/tree/master/tests 1、连接操作相关的命令 quit：关闭连接（connection） auth：简单密码认证 2、对value操作的命令 exists(key)： 确认一个key是否存在 del(key)： 删除一个key type(key)： 返回值的类型 keys(pattern)：返回满足给定pattern的所有key randomkey： 随机返回key空间的一个key rename(oldname, newname)：将key由oldname重命名为newname，若newname存在则删除newname表示的key dbsize： 返回当前数据库中key的数目 expire： 设定一个key的活动时间（s） ttl： 获得一个key的活动时间 select(index)： 按索引查询 move(key, dbindex)：将当前数据库中的key转移到有dbindex索引的数据库 flushdb： 删除当前选择数据库中的所有key flushall： 删除所有数据库中的所有key 3、对String操作的命令 set(key, value)： 给数据库中名称为key的string赋予值value get(key)： 返回数据库中名称为key的string的value getset(key, value)：给名称为key的string赋予上一次的value mget(key1, key2,…, key N)： 返回库中多个string（它们的名称为key1，key2…）的value setnx(key, value)： 如果不存在名称为key的string，则向库中添加string，名称为key，值为value setex(key, time, value)： 向库中添加string（名称为key，值为value）同时，设定过期时间time mset(key1, value1, key2, value2,…key N, value N)： 同时给多个string赋值，名称为key i的string赋值value i msetnx(key1, value1, key2, value2,…key N, value N)：如果所有名称为key i的string都不存在，则向库中添加string，名称key i赋值为value i incr(key)：名称为key的string增1操作 incrby(key, integer)：名称为key的string增加integer decr(key)：名称为key的string减1操作 decrby(key, integer)：名称为key的string减少integer append(key, value)：名称为key的string的值附加value substr(key, start, end)：返回名称为key的string的value的子串 4、对List操作的命令 rpush(key, value)：在名称为key的list尾添加一个值为value的元素 lpush(key, value)：在名称为key的list头添加一个值为value的 元素 llen(key)：返回名称为key的list的长度 lrange(key, start, end)：返回名称为key的list中start至end之间的元素（下标从0开始，下同） ltrim(key, start, end)：截取名称为key的list，保留start至end之间的元素 lindex(key, index)：返回名称为key的list中index位置的元素 lset(key, index, value)：给名称为key的list中index位置的元素赋值为value lrem(key, count, value)：删除count个名称为key的list中值为value的元素。 count为0，删除所有值为value的元素，count\u003e0从头至尾删除count个值为value的元素，count\u003c0从尾到头删除|count|个值为value的元素。 lpop(key)：返回并删除名称为key的list中的首元素 rpop(key)：返回并删除名称为key的list中的尾元素 blpop(key1, key2,… key N, timeout)：lpop命令的block版本。 即当timeout为0时，若遇到名称为key i的list不存在或该list为空，则命令结束。 如果timeout\u003e0，则遇到上述情况时，等待timeout秒，如果问题没有解决，则对keyi+1开始的list执行pop操作。 brpop(key1, key2,… key N, timeout)：rpop的block版本。参考上一命令。 rpoplpush(srckey, dstkey)：返回并删除名称为srckey的list的尾元素，并将该元素添加到名称为dstkey的list的头部 5、对Set操作的命令 sadd(key, member)：向名称为key的set中添加元素member srem(key, member) ：删除名称为key的set中的元素member spop(key) ：随机返回并删除名称为key的set中一个元素 smove(srckey, dstkey, member) ：将member元素从名称为srckey的集合移到名称为dstkey的集合 scard(key) ：返回名称为key的set的基数 sismember(key, member) ：测试member是否是名称为key的set的元素 sinter(key1, key2,…key N) ：求交集 sinterstore(dstkey, key1, key2,…key N) ：求交集并将交集保存到dstkey的集合 sunion(key1, key2,…key N) ：求并集 sunionstore(dstkey, key1, key2,…key N) ：求并集并将并集保存到dstkey的集合 sdiff(key1, key2,…key N) ：求差集 sdiffstore(dstkey, key1, key2,…key N) ：求差集并将差集保存到dstkey的集合 smembers(key) ：返回名称为key的set的所有元素 srandmember(key) ：随机返回名称为key的set的一个元素 6、对zset（sorted set）操作的命令 zadd(key, score, member)：向名称为key的zset中添加元素member，score用于排序。如该元素已存在，则根据score更新该元素的顺序。 zrem(key, member) ：删除名称为key的zset中的元素member zincrby(key, increment, member) ：如果在名称为key的zset中已经存在元素member，则该元素的score增加increment； 否则向集合中添加该元素，其score的值为increment zrank(key, member) ：返回名称为key的zset（元素已按score从小到大排序）中member元素的rank（即index，从0开始）， 若没有member元素，返回“nil” zrevrank(key, member) ：返回名称为key的zset（元素已按score从大到小排序）中member元素的rank（即index，从0开始）， 若没有member元素，返回“nil” zrange(key, start, end)：返回名称为key的zset（元素已按score从小到大排序）中的index从start到end的所有元素 zrevrange(key, start, end)：返回名称为key的zset（元素已按score从大到小排序）中的index从start到end的所有元素 zrangebyscore(key, min, max)：返回名称为key的zset中score \u003e= min且score \u003c= max的所有元素 zcard(key)：返回名称为key的zset的基数 zscore(key, element)：返回名称为key的zset中元素element的score zremrangebyrank(key, min, max)：删除名称为key的zset中rank \u003e= min且rank \u003c= max的所有元素 zremrangebyscore(key, min, max) ：删除名称为key的zset中score \u003e= min且score \u003c= max的所有元素 zunionstore / zinterstore(dstkeyN, key1,…,keyN, WEIGHTS w1,…wN, AGGREGATE SUM|MIN|MAX)：对N个zset求并集和交集， 并将最后的集合保存在dstkeyN中。对于集合中每一个元素的score，在进行AGGREGATE运算前，都要乘以对于的WEIGHT参数。 如果没有提供WEIGHT，默认为1。默认的AGGREGATE是SUM，即结果集合中元素的score是所有集合对应元素进行SUM运算的值， 而MIN和MAX是指，结果集合中元素的score是所有集合对应元素中最小值和最大值。 7、对Hash操作的命令 hset(key, field, value)：","date":"2019-11-20","objectID":"/posts/redis-cli/:1:2","series":null,"tags":["redis"],"title":"redis cli","uri":"/posts/redis-cli/#python-redis"}]