[{"categories":null,"content":"hello world ","date":"2021-12-07","objectID":"/posts/about/:0:0","series":null,"tags":null,"title":"关于我","uri":"/posts/about/#"},{"categories":[],"content":"github action + hugo / vuepress + github pages / gitee pages 自动构建发布 ","date":"2020-11-28","objectID":"/posts/github-action/:0:0","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#"},{"categories":[],"content":" GitHub pages + hugo新建两个仓库分别用于开发和部署(可用同一仓库) # example # dev repo -\u003e sunnyh1220/docsgo # deploy repo -\u003e sunnyh1220/posts (可选sunnyh1220.github.io 或 sunnyh1220/xxx的gh-pages分支 ) Deploy keys \u0026\u0026 Secrets ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\" # You will get 2 files: # gh-pages.pub (public key) # gh-pages (private key) 部署仓库设置Deploy keys: Settings -\u003e Deploy keys, 添加公钥, Allow write access勾上; 开发仓库设置Secrets: Setting -\u003e Secrets, 添加私钥, name为 ACTIONS_DEPLOY_KEY; 在开发仓库以下路径新建GitHub Actions部署脚本文件: `.github/workflow/hugo-ci.yml name: GitHub Pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.89.4' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: sunnyh1220/posts # github username publish_branch: gh-pages publish_dir: ./public hugo配置 # config.toml # 站点配置 baseURL = \"https://sunnyh1220.github.io/posts\" # 或 https://sunnyh1220.github.io 开发仓库提交后就会自动构建部署. ","date":"2020-11-28","objectID":"/posts/github-action/:0:1","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#github-pages--hugo"},{"categories":[],"content":" GitHub pages + vuepress仓库配置同上 VuePress配置 // .vuepress/config.js module.exports = { base: '/posts/' // repository name } GitHub Actions脚本文件 `.github/workflow/vuepress-ci.yml # This is a basic workflow to help you get started with Actions name: Blog CI # Controls when the action will run. on: # Triggers the workflow on push or pull request events but only for the master branch push: branches: [ master ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 # Runs a set of commands using the runners shell - name: Blog building run: | yarn install yarn docs:build - name: Blog Deploy uses: peaceiris/actions-gh-pages@v2.5.1 env: ACTIONS_DEPLOY_KEY: ${{ secrets.ACTIONS_DEPLOY_KEY }} EXTERNAL_REPOSITORY: sunnyh1220/posts # github deploy repository, PUBLISH_BRANCH: gh-pages # deploy branch PUBLISH_DIR: docs/.vuepress/dist ","date":"2020-11-28","objectID":"/posts/github-action/:0:2","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#github-pages--vuepress"},{"categories":[],"content":" gitee pages参考: https://github.com/yanglbme/gitee-pages-action ","date":"2020-11-28","objectID":"/posts/github-action/:0:3","series":null,"tags":[],"title":"Github Action","uri":"/posts/github-action/#gitee-pages"},{"categories":["Spark"],"content":" Spark ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:0","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#spark"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003efs.defaultFS\u003c/name\u003e \u003cvalue\u003ehdfs://master01:8020\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003ehadoop.tmp.dir\u003c/name\u003e \u003cvalue\u003e/home/sunyh/app/tmp\u003c/value\u003e \u003c/property\u003e ``` hdfs-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003cvalue\u003e1\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003emapreduce.framework.name\u003c/name\u003e \u003cvalue\u003eyarn\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e yarn-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.aux-serivices\u003c/name\u003e \u003cvalue\u003emapreduce_shuffle\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#1-install"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#11-java"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#12-scala"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#13-hadoop"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#14-maven"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#15-spark"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#151-源码编译"},{"categories":["Spark"],"content":" 1. installenv : centos7 1.1 java卸载centos7自带openjdk,安装oracle的jdk sh-4.2$ rpm -qa|grep jdk java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 copy-jdk-configs-3.3-10.el7_5.noarch # uninstall sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 sh-4.2$ rpm -qa|grep jdk copy-jdk-configs-3.3-10.el7_5.noarch tar -zxvf jdk-8u251-linux-x64.tar.gz -C ~/app/ 修改环境变量: vim ~/.bash_profile export JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 export PATH=$JAVA_HOME/bin:$PATH source ~/.bash_profile 1.2 scalaspark与scala版本对应参考: https://spark.apache.org/docs/latest/index.html scala下载: https://www.scala-lang.org/download/2.12.11.html tar -zxvf scala-2.11.12.tgz -C ~/app/ env: export SCALA_HOME=/home/sunyh/app/scala-2.12.11 export PATH=$SCALA_HOME/bin:$PATH https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz 1.3 hadoopcdh版本: http://archive.cloudera.com/cdh5/cdh/5/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app/ env: export HADOOP_HOME=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2 export PATH=$HADOOP_HOME/bin:$PATH 配置修改: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop hadoop-env.sh 修改JAVA_HOME core-site.xml fs.defaultFS hdfs://master01:8020 hadoop.tmp.dir /home/sunyh/app/tmp ``` hdfs-site.xml dfs.replication 1 mapred-site.xml.template cp mapred-site.xml.template mapred-site.xml mapreduce.framework.name yarn yarn-site.xml yarn.nodemanager.aux-serivices mapreduce_shuffle 格式化: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/bin ./hdfs namenode -formate 启动hdfs: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-dfs.sh 查看启动状态: jps 测试: hadoop fs -ls / hadoop fs -mkdir /test cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2 hadoop fs -put README.txt /test/ hadoop fs -ls /test hadoop fs -text /test/README.txt web: master01:50070 启动YARN: cd /home/sunyh/app/hadoop-2.6.0-cdh5.16.2/sbin ./start-yarn.sh web: master01:8088 1.4 mavenhttps://maven.apache.org/download.cgi https://spark.apache.org/docs/latest/building-spark.html tar -zxvf apache-maven-3.5.4-bin.tar.gz -C ~/app/ env: export MAVEN_HOME=/home/sunyh/app/apache-maven-3.5.4 export PATH=$MAVEN_HOME/bin:$PATH 1.5 spark 1.5.1 源码编译编译没成功 package type选择source type. https://spark.apache.org/docs/latest/building-spark.html tar -zxvf spark-2.4.5.tgzbash export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" cd /home/sunyh/software/spark-2.4.5 ./dev/make-distribution.sh --name 2.6.0-cdh5.16.2 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.16.2 1.5.2 tar包解压 tar -zxvf spark-2.4.5-bin-hadoop2.6.tgz -C ~/app/ export SPARK_HOME=/home/sunyh/app/spark-2.4.5-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$PATH ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:1","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#152-tar包解压"},{"categories":["Spark"],"content":" 2. Spark RDDResilient Distributed Dataset https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala RDD编程 Parallelized Collections External Datasets PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. RDD Operations transformations actions RDD Persistence ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:2","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#2-spark-rdd"},{"categories":["Spark"],"content":" 2. Spark RDDResilient Distributed Dataset https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala RDD编程 Parallelized Collections External Datasets PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. RDD Operations transformations actions RDD Persistence ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:2","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#rdd编程"},{"categories":["Spark"],"content":" 3. PySparkPyCharm环境配置(local 本地测试): python intercepter python structure 添加/home/sunyh/app/spark-2.4.5-bin-hadoop2.6/python/lib下的两个zip包 Run Configurations env 添加PYTHONPATH 测试: from pyspark import SparkConf,SparkContext if __name__ == \"__main__\": # conf = SparkConf().setMaster(\"local[2]\").setAppName(\"spark0526\") conf = SparkConf() sc = SparkContext(conf=conf) data = [1, 2, 3, 4, 5] distData = sc.parallelize(data) print(distData.collect()) sc.stop() 提交pyspark应用程序: 参考:https://spark.apache.org/docs/latest/submitting-applications.html spark-submit --master local[2] --name spark0526 /home/sunyh/py_project/spark_test/test.py spark-submit --help ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:3","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#3-pyspark"},{"categories":["Spark"],"content":" 4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \u003cinput\u003e\", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalonehttps://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarnhttps://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId \u003capp ID\u003e yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml \u003cproperty\u003e \u003cname\u003eyarn.log-aggregation-enable\u003c/name\u003e \u003cvalue\u003etrue\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds\u003c/name\u003e \u003cvalue\u003e3600\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.remote-app-log-dir\u003c/name\u003e \u003cvalue\u003e/tmp/logs\u003c/value\u003e \u003c/property\u003e 重启yarn,测试 yarn logs --applicationId application_159056671","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#4-spark运行模式"},{"categories":["Spark"],"content":" 4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalonehttps://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarnhttps://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml yarn.log-aggregation-enable true yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 3600 yarn.nodemanager.remote-app-log-dir /tmp/logs 重启yarn,测试 yarn logs --applicationId application_159056671","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#41-local"},{"categories":["Spark"],"content":" 4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalonehttps://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarnhttps://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml yarn.log-aggregation-enable true yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 3600 yarn.nodemanager.remote-app-log-dir /tmp/logs 重启yarn,测试 yarn logs --applicationId application_159056671","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#42-standalone"},{"categories":["Spark"],"content":" 4. Spark运行模式 # spark_op.py 统计单词个数 import sys from pyspark import SparkConf, SparkContext if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) conf = SparkConf() sc = SparkContext(conf=conf) def print_result(): counts = sc.textFile(sys.argv[1]) \\ .flatMap(lambda line: line.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(lambda a, b: a + b) output = counts.collect() for (word, count) in output: print('%s: %i' % (word, count)) print_result() sc.stop() hello.txt hello world hello spark welcome to beijing 4.1 Local # 单文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 4.2 Standalonehttps://spark.apache.org/docs/latest/spark-standalone.html 修改配置文件: # cd $SPARK_HOME/conf cd /home/sunyh/app/spark-2.4.5-bin-hadoop2.6/conf cp slaves.template slaves # 修改spark slave节点名 vim slaves 如果多台机器,每台机器都在相同路径下部署spark; cd $SPARK_HOME/conf cp spark-env.sh.template spark-env.sh vim spark-env.sh # 添加java环境变量 JAVA_HOME=/home/sunyh/app/jdk1.8.0_251 启动spark: cd $SPARK_HOME/sbin # 可以使用start-master.sh / start-slave.sh 分别启动 ./start-all.sh 检查是否启动成功: jps HDFS: NameNode / DataNode YARN: ResourceManager / NodeManager Spark Standalone: Master / Worker 启动日志: 测试: # 单文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data/hello.txt # 多文件 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ file:///home/sunyh/py_project/spark_test/data 使用standalone模式而且节点数大于1,使用本地文件测试,必须保证每个节点都有测试文件. # 测试文件上传到hdfs hadoop fs -put /home/sunyh/py_project/spark_test/data/hello.txt /test.txt # 查看文件 hadoop fs -text /test.txt # 测试 spark-submit --master spark://master01:7077 --name spark_standalone \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 4.3 Yarnhttps://spark.apache.org/docs/latest/running-on-yarn.html spark仅作为客户端,然后把作业提交到yarn执行; yarn vs standalone: ​ yarn: 只需一个spark节点,不需要spark集群(不用启动master,worker) ​ standalone: spark集群每个节点都需要部署spark,然后启动spark集群(master,worker) yarn模式配置: cd $SPARK_HOME/conf vim spark-env.sh # 添加HADOOP_CONF_DIR环境变量 HADOOP_CONF_DIR=/home/sunyh/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop 测试: spark-submit --master yarn --name spark_yarn \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 yarn部署模式(deploy-mode): client(默认)和cluster ​ client: 提交作业的进程不能停止 ​ cluster: 提交完作业,提交作业端断开,所以pyspark/spark-shell/spark-sql等交互式运行程序不能用cluster模式 There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN. # cluster spark-submit --master yarn --name spark_yarn_cluster --deploy-mode cluster \\ /home/sunyh/py_project/spark_test/spark_op.py \\ hdfs://master01:8020/test.txt \\ hdfs://master01:8020/test/output #输出文件 查看yarn application 日志: yarn logs --applicationId yarn logs --applicationId application_1590484746232_0003 Q: Log aggregation has not completed or is not enabled.(日志聚合功能没开启) A: 修改yarn-site.xml yarn.log-aggregation-enable true yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds 3600 yarn.nodemanager.remote-app-log-dir /tmp/logs 重启yarn,测试 yarn logs --applicationId application_159056671","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:4","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#43-yarn"},{"categories":["Spark"],"content":" 5. Monitoringhttps://spark.apache.org/docs/latest/monitoring.html 配置修改: cd $SPARK_HOME/conf cp spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf # 设置spark.eventLog.enabled和spark.eventLog.dir vim spark-env.sh # 修改SPARK_HISTORY_OPTS # SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=hdfs://master01:8020/spark-logs\" 启动history server: cd $SPARK_HOME/sbin ./start-history-server.sh 不同模式提交任务查看history server: 任务详细信息: 任务日志以json保存在配置的hdfs: ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:5","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#5-monitoring"},{"categories":["Spark"],"content":" 6. Spark调优https://spark.apache.org/docs/latest/tuning.html Data Serialization Memory Tuning Broadcasting Large Variables https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables Data Locality ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:6","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#6-spark调优"},{"categories":["Spark"],"content":" 7. Spark SQL https://spark.apache.org/sql/ https://spark.apache.org/docs/latest/sql-programming-guide.html Spark SQL is Apache Spark’s module for working with structured data. Spark SQL提供的操作数据的方式: SQL, DataFrame API, Dataset API(暂时不支持python) Dataset A Dataset is a distributed collection of data. [[Row]]的数据集 DataFrame A DataFrame is a Dataset organized into named columns. 以列(列名,列类型,列值)的的形式构成的分布式数据集. DataFrame = RDD[Row] + shcema 测试文件: spark_sql_op.py from pyspark.sql import SparkSession, Row # Import data types from pyspark.sql.types import * def base_test(spark): # spark is an existing SparkSession df = spark.read.json(\"file:///home/sunyh/app/spark-2.4.5-bin-hadoop2.6/examples/src/main/resources/people.json\") # Displays the content of the DataFrame to stdout df.show() # +----+-------+ # | age| name| # +----+-------+ # |null|Michael| # | 30| Andy| # | 19| Justin| # +----+-------+ # spark, df are from the previous example # Print the schema in a tree format df.printSchema() # root # |-- age: long (nullable = true) # |-- name: string (nullable = true) # Select only the \"name\" column df.select(\"name\").show() # +-------+ # | name| # +-------+ # |Michael| # | Andy| # | Justin| # +-------+ # Select everybody, but increment the age by 1 df.select(df['name'], df['age'] + 1).show() # +-------+---------+ # | name|(age + 1)| # +-------+---------+ # |Michael| null| # | Andy| 31| # | Justin| 20| # +-------+---------+ # Select people older than 21 df.filter(df['age'] \u003e 21).show() # +---+----+ # |age|name| # +---+----+ # | 30|Andy| # +---+----+ # Count people by age df.groupBy(\"age\").count().show() # +----+-----+ # | age|count| # +----+-----+ # | 19| 1| # |null| 1| # | 30| 1| # +----+-----+ def schema_inference_example(spark): \"\"\" RDD --\u003e DataFrame 自动推导Schema :param spark: \"\"\" sc = spark.sparkContext # Load a text file and convert each line to a Row. lines = sc.textFile(\"file:///home/sunyh/app/spark-2.4.5-bin-hadoop2.6/examples/src/main/resources/people.txt\") parts = lines.map(lambda l: l.split(\",\")) people = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) # Infer the schema, and register the DataFrame as a table. schemaPeople = spark.createDataFrame(people) schemaPeople.printSchema() schemaPeople.createOrReplaceTempView(\"people\") # SQL can be run over DataFrames that have been registered as a table. teenagers = spark.sql(\"SELECT name FROM people WHERE age \u003e= 13 AND age \u003c= 19\") # The results of SQL queries are Dataframe objects. # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`. teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect() for name in teenNames: print(name) # Name: Justin def schema_programmatic_example(spark): \"\"\" RDD --\u003e DataFrame 编程实现Schema :param spark: \"\"\" sc = spark.sparkContext # Load a text file and convert each line to a Row. lines = sc.textFile(\"file:///home/sunyh/app/spark-2.4.5-bin-hadoop2.6/examples/src/main/resources/people.txt\") parts = lines.map(lambda l: l.split(\",\")) # Each line is converted to a tuple. people = parts.map(lambda p: (p[0], p[1].strip())) # The schema is encoded in a string. schemaString = \"name age\" fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()] schema = StructType(fields) # Apply the schema to the RDD. schemaPeople = spark.createDataFrame(people, schema) # Creates a temporary view using the DataFrame schemaPeople.createOrReplaceTempView(\"people\") # SQL can be run over DataFrames that have been registered as a table. results = spark.sql(\"SELECT name FROM people\") results.show() # +-------+ # | name| # +-------+ # |Michael| # | Andy| # | Justin| # +-------+ if __name__ == '__main__': spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL basic example\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate() # base_test(spark=spark) # schema_inference_example(spark) schema_programmatic_example(spark) spark.stop() spark-submit --master local[2] --name spark_local \\ /home/sunyh/py_project/spark_test/s","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:7","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#7-spark-sql"},{"categories":["Spark"],"content":" 8. Spark Streaming https://spark.apache.org/streaming/ https://spark.apache.org/docs/latest/streaming-programming-guide.html Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. 常用实时流处理框架: Storm: 真正实时流处理 Spark Streaming: 不是真正的实时流处理,而是mini batch操作 Flink Kafka Stream DStream Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. Internally, a DStream is represented as a sequence of RDDs. https://github.com/apache/spark/blob/v2.4.5/examples/src/main/python/streaming/network_wordcount.py nc -lk 9999 cd $SPARK_HOME ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999 ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:8","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#8-spark-streaming"},{"categories":["Spark"],"content":" 9. MLlib https://spark.apache.org/mllib/ https://spark.apache.org/docs/latest/ml-guide.html spark.ml –\u003e DataFrame-based API spark.mllib –\u003e RDD-based API ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:9","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#9-mllib"},{"categories":["Spark"],"content":" 10. GraphX https://spark.apache.org/graphx/ https://spark.apache.org/docs/latest/graphx-programming-guide.html ","date":"2020-05-25","objectID":"/posts/spark2_basics/:1:10","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#10-graphx"},{"categories":["Spark"],"content":" Spark3 参考: https://spark.apache.org/docs/3.0.0-preview2/index.html https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/ https://github.com/rapidsai/spark-examples run a sample Apache Spark Python application that runs on NVIDIA GPUs ","date":"2020-05-25","objectID":"/posts/spark2_basics/:2:0","series":null,"tags":["Spark2"],"title":"Spark2环境搭建与使用","uri":"/posts/spark2_basics/#spark3"},{"categories":["NVIDIA","GPU"],"content":" Reference: Hyper-Q Example MULTI-PROCESS SERVICE IMPROVING GPU UTILIZATION WITH MULTI-PROCESS SERVICE (MPS) 多个docker容器如何共享GPU 基于Volta MPS执行资源配置下的多容器共享GPU性能测试 MPS ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:0","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#"},{"categories":["NVIDIA","GPU"],"content":" 1. CUDA context 类似于CPU进程上下文,表示与特定进程关联的所有状态 从CPU端分配的GPU上的Global memory (cudaMalloc/cudaMallocManaged) Kernel函数中定义和分配的堆栈空间，例如local memory CUDA streams / events 对象 代码模块(*.cubin, *.ptx) 不同的进程有自己的CUDA context 每个context有自己的地址空间，并且不能访问其他CUDA context的地址空间 时间片轮转调度,相当于串行执行,每个时刻只有一个进程在GPU上执行,吞吐率不会发生变化,而且由于上下文context切换的开销,延迟增加的同时,吞吐量反而是下降的. ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:1","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#1-cuda-context"},{"categories":["NVIDIA","GPU"],"content":" 2. Hyper-Q (Hyper Queue) 允许多个CPU 线程或进程同时加载任务到一个GPU上,实现CUDA kernels的并发执行 –- 硬件特性 支持的连接类型 Multi cuda streams Multi cpu threads Multi cpu processes——MPS 管理可并发的最大连接数 # (默认是8) CUDA_DEVICE_MAX_CONNECTIONS = 32 好处 增加GPU利用率（utilization）和占用率（occupancy） 减少CPU空闲时间 增加吞吐率并减少延 使用限制 当kernel A正在执行时, 只有当GPU上任意SM上有足够的资源( 寄存器, 共享内存, 线程块槽位等等)来执行kernel B中的1个线程块时， kernel B才会被发射 要求GPU计算能力大于等于3.5 最大连接数限制： 32 示例代码 $CUDA_PATH/samples/6_Advanced/simpleHyperQ ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:2","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#2-hyper-q-hyper-queue"},{"categories":["NVIDIA","GPU"],"content":" 3. MPS - Multi-Process Service，多进程服务 什么是MPS 一组可替换的，二进制兼容的CUDA API实现，包括：守护进程,服务进程,用户运行时 利用GPU上的Hyper-Q 能力 允许多个CPU进程共享同一GPU context 允许不同进程的kernel和memcpy操作在同一GPU上并发执行，以实现最大化 GPU利用率. 好处 提升GPU利用率（时间上）和占用率（空间上） 减少GPU上下文切换时间 减少GPU上下文存储空 使用限制 操作系统: 仅支持linux GPU版本： 大于等于CC 3.5 CUDA版本：大于等于5.5 最大用户连接数量： Pascal及之前GPU: 16 Volta及之后GPU: 48 启动 设置GPU compute mode 为 exclusive mode (非必须，但推荐设置) sudo nvidia-smi -i 0 -c EXCLUSIVE_PROCESS 启动MPS 守护进程 # 指定要启动MPS的GPU,不指定默认对所有GPU生效 export CUDA_VISIBLE_DEVICES=0 # cuda 7.0 以后非必须 export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps # cuda 7.0 以后非必须 export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log nvidia-cuda-mps-control –d 查看MPS 守护进程是否正在运行 ps -ef | grep mps 运行应用程序 mpirun –np 4 ./test 应用程序运行前: 应用程序运行后: 停止 echo quit | nvidia-cuda-mps-control sudo nvidia-smi -i 0 -c 0 监视 nvidia-smi 分析 nvprof --profile-all-processes -o output.%p mpirun –np 4 ./test ","date":"2020-04-15","objectID":"/posts/cuda_mps/:0:3","series":null,"tags":["CUDA","MPS"],"title":"CUDA多进程服务MPS","uri":"/posts/cuda_mps/#3-mps---multi-process-service多进程服务"},{"categories":["AI","分布式训练"],"content":" Reference: https://github.com/horovod/horovod http://www.dataguru.cn/article-14746-1.html Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. ","date":"2020-04-15","objectID":"/posts/horovod_basics/:0:0","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#"},{"categories":["AI","分布式训练"],"content":" 1. 分布式深度学习原理深度学习训练的算法叫做反向传播。即通过神经网络得到预测结果，把预测结果跟标注Label进行比对，发现误差；然后得到神经网络里每个神经元权重导数；接着通过算法得到每个神经元导数，再更新神经元的权重以得到更好的神经元网络，周而复始迭代训练，使得误差减少。 神经网络推理能力随着规模、复杂度增加，能力会极大的增强。但从计算能力角度来说又出现了新问题：很多时候大规模神经网络很难在单个/单点计算单元里面运行，这会导致计算很慢，以至无法运行大规模数据。所以人们提出两种深度学习的基本方法以解决这个问题。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:0","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#1-分布式深度学习原理"},{"categories":["AI","分布式训练"],"content":" 1.1 深度学习的两种基本方法 1.1.1 模型并行把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 1.1.2 数据并行让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 数据并行原理: ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#11-深度学习的两种基本方法"},{"categories":["AI","分布式训练"],"content":" 1.1 深度学习的两种基本方法 1.1.1 模型并行把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 1.1.2 数据并行让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 数据并行原理: ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#111-模型并行"},{"categories":["AI","分布式训练"],"content":" 1.1 深度学习的两种基本方法 1.1.1 模型并行把复杂的神经网络拆分，分布在计算单元或者GPU里面进行学习，让每个GPU同步进行计算。这个方法通常用在模型比较复杂的情况下。 1.1.2 数据并行让每个机器里都有一个完整模型，然后把数据切分成n块，把n块分发给每个计算单元，每个计算单元独自计算出自己的梯度。同时每个计算单元的梯度会进行平均、同步，同步后的梯度可以在每个节点独立去让它修正模型，整个过程结束后每个节点会得到同样的模型。这个方法可以让能够处理的数据量增加，变成了原来的n倍。 数据并行原理: ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#112-数据并行"},{"categories":["AI","分布式训练"],"content":" 1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#12-实现数据并行的两种工程方法"},{"categories":["AI","分布式训练"],"content":" 1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#121-参数服务器-parameter-server"},{"categories":["AI","分布式训练"],"content":" 1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#122-ring-allreduce"},{"categories":["AI","分布式训练"],"content":" 1.2 实现数据并行的两种工程方法 1.2.1 参数服务器 (Parameter Server) 在计算单元以外加设新的服务器叫做参数服务器。每次训练的时候每个计算单元把梯度发送给参数服务器，服务器把他们进行汇总计算平均值，把平均值返回到每个计算单元，这样每个计算单元就同步了。 1.2.2 Ring-AllReduce 它是从高性能计算集合通信找到的想法。做法是把每个计算单元构建成一个环，要做梯度平均的时候每个计算单元先把自己梯度切分成N块，然后发送到相邻下一个模块。现在有N个节点，那么N-1次发送后就能实现所有节点掌握所有其他节点的数据。这个方法被证明是一个带宽最优算法。 1.2.3 Parameter Server与Ring-AllReduce对比 参数服务器的做法理论容错性比较强，因为每个节点相互之间没有牵制，互相没有关联，它只是需要跟参数服务器本身进行通信，就可以运作了。缺点是有额外的网络开销，扩展效率会受到影响。 Ring-AllReduce优点非常明显，性能非常好，如果在大规模分布式训练时候资源利用率相当高，网络占用是最优的。它的缺点是在工程上的缺点，容错性较差，很多实现都是用MPI实现（MPI本身并不是为容错设计的，它更偏向于照顾高性能的计算）。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:1:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#123-parameter-server与ring-allreduce对比"},{"categories":["AI","分布式训练"],"content":" 2. HorovodHorovod是基于Ring-AllReduce方法(参考了百度解决方案)的分布式深度学习插件，以支持多种流行架构包括TensorFlow、Keras、PyTorch等。这样平台开发者只需要为Horovod进行配置，而不是对每个架构有不同的配置方法。 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:0","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#2-horovod"},{"categories":["AI","分布式训练"],"content":" 2.1 Horovod特点 基于Ring-AllReduce方法 一套插件,支持多种流行架构:Tensorflow, Keras, PyTorch, MxNet 通用环境配置,独立于架构 容易安装与使用 pip install horovod 单机程序稍加修改即可实现分布式学习 与Apache Spark集成 性能高效 支持MPI, NCCL, RDMA, GPUDirect ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:1","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#21-horovod特点"},{"categories":["AI","分布式训练"],"content":" 2.2 Horovod内部架构 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:2","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#22-horovod内部架构"},{"categories":["AI","分布式训练"],"content":" 2.4 Horovod使用方法单机Keras训练脚本: 用Keras定义它的model X-train、Y-train，定义训练样本和测试样本 optimizer，即选定优化器 model.compile model. fit Horovod+Keras分布式训练脚本: 初始化,指定GPU; 定义优化器的时候，优化器重要参数是学习率(lr) , lr * hvd.size(); 使用Horovod的DistributedOptimizer将优化器包裹，抽象了Horovod需要进行的梯度平均的逻辑; 加callback,让Horovod把每次训练的初始状态广播到每个节点，这样保证每个节点从同一个地方开始; epochs平均到每台机器 启动分布式训练: # 单机多卡 horovodrun -np 4 -H localhost:4 python train.py # 多级多卡 horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:3","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#24-horovod使用方法"},{"categories":["AI","分布式训练"],"content":" 2.5 高级功能 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:4","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#25-高级功能"},{"categories":["AI","分布式训练"],"content":" 2.6 新功能预览 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:5","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#26-新功能预览"},{"categories":["AI","分布式训练"],"content":" 2.7 容器中使用Horovod 2.7.1 docker reference: https://github.com/horovod/horovod https://hub.docker.com/r/horovod/horovod/tags 2.7.2 k8s reference: https://github.com/helm/charts/tree/master/stable/horovod https://hub.helm.sh/charts/stable/horovod/1.0.0 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:6","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#27-容器中使用horovod"},{"categories":["AI","分布式训练"],"content":" 2.7 容器中使用Horovod 2.7.1 docker reference: https://github.com/horovod/horovod https://hub.docker.com/r/horovod/horovod/tags 2.7.2 k8s reference: https://github.com/helm/charts/tree/master/stable/horovod https://hub.helm.sh/charts/stable/horovod/1.0.0 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:6","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#271-docker"},{"categories":["AI","分布式训练"],"content":" 2.7 容器中使用Horovod 2.7.1 docker reference: https://github.com/horovod/horovod https://hub.docker.com/r/horovod/horovod/tags 2.7.2 k8s reference: https://github.com/helm/charts/tree/master/stable/horovod https://hub.helm.sh/charts/stable/horovod/1.0.0 ","date":"2020-04-15","objectID":"/posts/horovod_basics/:2:6","series":null,"tags":["Horovod","分布式训练"],"title":"Horovod基本原理与使用","uri":"/posts/horovod_basics/#272-k8s"},{"categories":["NVIDIA","GPU"],"content":" 参考: https://docs.nvidia.com/isaac/isaac/doc/overview.html https://developer.nvidia.com/isaac-sdk ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:0:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#"},{"categories":["NVIDIA","GPU"],"content":" 1. Isaac Engineapplication framework Isaac Engine是一个软件框架，可轻松构建模块化机器人应用程序。它为智能机器人提供了高性能的数据处理和深度学习。在Isaac机器人引擎上开发的机器人应用程序可以在NVIDIA®Jetson AGX Xavier™和NVIDIA®Jetson Nano™等边缘设备以及具有离散NVIDIA®GPU的工作站（例如T4）上无缝运行。 ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:1:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#1-isaac-engine"},{"categories":["NVIDIA","GPU"],"content":" 2. Isaac GEMspackages with high-performance robotics algorithms GEMs是用于感应，规划或驱动的模块化功能，可以轻松插入机器人应用程序中。例如，开发人员可以添加障碍物检测，立体声深度估计或人类语音识别，以丰富其机器人用例。 ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:2:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#2-isaac-gems"},{"categories":["NVIDIA","GPU"],"content":" 3. Isaac Appsreference applications ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:3:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#3-isaac-apps"},{"categories":["NVIDIA","GPU"],"content":" 4. Isaac Sima powerful simulation platform Isaac Sim是虚拟机器人实验室和高保真3D世界模拟器，可通过降低成本和风险来加速研究，设计和开发。 ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:4:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#4-isaac-sim"},{"categories":["NVIDIA","GPU"],"content":" 5. Dev","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:0","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#5-dev"},{"categories":["NVIDIA","GPU"],"content":" 5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u \u003cjetson_username\u003e -h \u003cjetson_ip\u003e engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#51-setup"},{"categories":["NVIDIA","GPU"],"content":" 5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-dependencies-on-the-desktop"},{"categories":["NVIDIA","GPU"],"content":" 5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-bazelhttpsdocsbazelbuildversionsmasterinstall-ubuntuhtml"},{"categories":["NVIDIA","GPU"],"content":" 5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-nvidia-gpu-driver"},{"categories":["NVIDIA","GPU"],"content":" 5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#installing-cuda100httpsdevelopernvidiacomcuda-100-download-archivetarget_oslinuxtarget_archx86_64target_distroubuntutarget_version1804target_typerunfilelocal"},{"categories":["NVIDIA","GPU"],"content":" 5.1 Setup Env: Ubuntu18.04.1 (Nvidia Driver 440, cuda10.0, GPU算力 \u003e= 3.5) Nano: Jetpack4.3 下载Isaac SDK和Isaac Sim, 分别解压到 ~/isaac_sdk 和 ~/isaac_sim_unity3d 添加镜像源: sudo vi /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse ideb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse 安装pip3 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py # 修改为豆瓣源 sudo pip3 config set global.index-url https://pypi.doubanio.com/simple/ Installing Dependencies on the Desktop cd ~/isaac_sdk engine/build/scripts/install_dependencies.sh Installing Dependencies on Robots: cd ~/isaac_sdk # engine/build/scripts/install_dependencies_jetson.sh -u -h engine/build/scripts/install_dependencies_jetson.sh -u nona -h 192.168.20.155 Installing Bazel sudo apt install curl gnupg curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update \u0026\u0026 sudo apt install bazel # Ubuntu 18.04 (LTS) uses OpenJDK 11 by default: sudo apt install openjdk-11-jdk Installing NVIDIA GPU Driver sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-driver-440 Installing CUDA10.0 # 下载.run脚本并运行, 注: 只安装cuda tool kit, 不安装driver wget https://developer.download.nvidia.cn/compute/cuda/10.0/secure/Prod/local_installers/cuda_10.0.130_410.48_linux.run sudo sh cuda_10.0.130_410.48_linux.run 在vim ~/.bashrc的最后加上以下配置信息 export CUDA_HOME=/usr/local/cuda-10.0 export LD_LIBRARY_PATH=${CUDA_HOME}/lib64 export PATH=${CUDA_HOME}/bin:${PATH} 使用命令source ~/.bashrc使配置生效 Install Unity Editor for Editor Mode https://forum.unity.com/threads/unity-hub-v-1-3-2-is-now-available.594139/ wget https://public-cdn.cloud.unity3d.com/hub/prod/UnityHub.AppImage chmod +x UnityHub.AppImage ./UnityHub.AppImage When the Unity Hub application opens, follow these steps to install the Unity Editor: Click the person icon in the upper right and select Sign in. Sign in with your Unity ID. Select Installs on the left, then select Add. In the Add Unity Version popup window, install 2019.3.x (no modules are required). The sample project for IsaacSim Unity3D is created with 2019.3.0f6, so any newer 2019.3 version can be used. ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:1","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#install-unity-editor-for-editor-mode"},{"categories":["NVIDIA","GPU"],"content":" 5.2 运行Isaac SDK示例App 5.2.1 stereo_dummy sample application cd ~/isaac_sdk bazel build //apps/samples/stereo_dummy bazel run //apps/samples/stereo_dummy 应用运行后,可在浏览器打开http://localhost:3000 将应用部署运行在Jetson上: # 配置desktop免密登录Jetson ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub nona@192.168.20.155 # deploy the package to the robot cd ~/isaac_sdk # ./engine/build/deploy.sh --remote_user \u003cusername_on_robot\u003e -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h \u003crobot_ip\u003e ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 在Jetson上运行 : # ssh ROBOTUSER@ROBOTIP ssh nona@192.168.20.155 # deep为desktop的用户名 cd deploy/deep/stereo_dummy-pkg ./apps/samples/stereo_dummy/stereo_dummy # 应用运行后浏览器打开192.168.20.155:3000 也可以部署到Jetson同时运行App,加 --run ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 --run 5.2.2 v4l2_camera sample application bazel build //apps/samples/v4l2_camera bazel run //apps/samples/v4l2_camera ./engine/build/deploy.sh --remote_user nona -p //apps/samples/v4l2_camera:v4l2_camera-pkg -d jetpack43 -h 192.168.20.155 --run ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:2","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#52-运行isaac-sdk示例app"},{"categories":["NVIDIA","GPU"],"content":" 5.2 运行Isaac SDK示例App 5.2.1 stereo_dummy sample application cd ~/isaac_sdk bazel build //apps/samples/stereo_dummy bazel run //apps/samples/stereo_dummy 应用运行后,可在浏览器打开http://localhost:3000 将应用部署运行在Jetson上: # 配置desktop免密登录Jetson ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub nona@192.168.20.155 # deploy the package to the robot cd ~/isaac_sdk # ./engine/build/deploy.sh --remote_user -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 在Jetson上运行 : # ssh ROBOTUSER@ROBOTIP ssh nona@192.168.20.155 # deep为desktop的用户名 cd deploy/deep/stereo_dummy-pkg ./apps/samples/stereo_dummy/stereo_dummy # 应用运行后浏览器打开192.168.20.155:3000 也可以部署到Jetson同时运行App,加 --run ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 --run 5.2.2 v4l2_camera sample application bazel build //apps/samples/v4l2_camera bazel run //apps/samples/v4l2_camera ./engine/build/deploy.sh --remote_user nona -p //apps/samples/v4l2_camera:v4l2_camera-pkg -d jetpack43 -h 192.168.20.155 --run ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:2","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#521--stereo_dummy-sample-application"},{"categories":["NVIDIA","GPU"],"content":" 5.2 运行Isaac SDK示例App 5.2.1 stereo_dummy sample application cd ~/isaac_sdk bazel build //apps/samples/stereo_dummy bazel run //apps/samples/stereo_dummy 应用运行后,可在浏览器打开http://localhost:3000 将应用部署运行在Jetson上: # 配置desktop免密登录Jetson ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub nona@192.168.20.155 # deploy the package to the robot cd ~/isaac_sdk # ./engine/build/deploy.sh --remote_user -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 在Jetson上运行 : # ssh ROBOTUSER@ROBOTIP ssh nona@192.168.20.155 # deep为desktop的用户名 cd deploy/deep/stereo_dummy-pkg ./apps/samples/stereo_dummy/stereo_dummy # 应用运行后浏览器打开192.168.20.155:3000 也可以部署到Jetson同时运行App,加 --run ./engine/build/deploy.sh --remote_user nona -p //apps/samples/stereo_dummy:stereo_dummy-pkg -d jetpack43 -h 192.168.20.155 --run 5.2.2 v4l2_camera sample application bazel build //apps/samples/v4l2_camera bazel run //apps/samples/v4l2_camera ./engine/build/deploy.sh --remote_user nona -p //apps/samples/v4l2_camera:v4l2_camera-pkg -d jetpack43 -h 192.168.20.155 --run ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:2","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#522--v4l2_camera-sample-application"},{"categories":["NVIDIA","GPU"],"content":" 5.3 运行Isaac Sim示例App 5.3.1 Play Mode 5.3.1.1 Warehouse Navigation # start the simulator with the “small_warehouse” scene cd ~/isaac_sim_unity3d/builds ./sample.x86_64 --scene small_warehouse -logFile - # run the Isaac SDK application with the Carter navigation stack cd ~/isaac_sdk bazel run //apps/navsim:navsim_navigate ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:3","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#53-运行isaac-sim示例app"},{"categories":["NVIDIA","GPU"],"content":" 5.3 运行Isaac Sim示例App 5.3.1 Play Mode 5.3.1.1 Warehouse Navigation # start the simulator with the “small_warehouse” scene cd ~/isaac_sim_unity3d/builds ./sample.x86_64 --scene small_warehouse -logFile - # run the Isaac SDK application with the Carter navigation stack cd ~/isaac_sdk bazel run //apps/navsim:navsim_navigate ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:3","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#531-play-mode"},{"categories":["NVIDIA","GPU"],"content":" 5.3 运行Isaac Sim示例App 5.3.1 Play Mode 5.3.1.1 Warehouse Navigation # start the simulator with the “small_warehouse” scene cd ~/isaac_sim_unity3d/builds ./sample.x86_64 --scene small_warehouse -logFile - # run the Isaac SDK application with the Carter navigation stack cd ~/isaac_sdk bazel run //apps/navsim:navsim_navigate ","date":"2020-04-15","objectID":"/posts/isaac_sdk/:5:3","series":null,"tags":["Isaac"],"title":"NVIDIA Isaac SDK","uri":"/posts/isaac_sdk/#5311-warehouse-navigation"},{"categories":["CloudNative"],"content":" Kubeadm 安装","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:0","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#kubeadm-安装"},{"categories":["CloudNative"],"content":" 1. docker install sudo apt-get install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" apt-cache madison docker-ce apt-cache madison docker-ce-cli sudo apt-get install docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic containerd.io sudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://registry.docker-cn.com\"] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 开机自启: systemctl enable docker \u0026\u0026 systemctl start docker # systemctl enable kubelet \u0026\u0026 systemctl start kubelet ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:1","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#1-docker-install"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod \u003cpod_name\u003e 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes \u003cnode-name\u003e node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes \u003cnode-name\u003e node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#2-k8s-install-by-kubeadm"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl # export KUBECONFIG=/etc/kubernetes/admin.conf sudo systemctl daemon-reload sudo systemctl restart kubelet 卸载之前的版本 sudo apt-get remove -y kubelet kubeadm kubectl sudo apt autoremove # rm -rf ~/.kube 安装指定版本 # 比如 1.13.12 sudo apt-get install -y kubelet=1.13.12-00 kubeadm=1.13.12-00 kubectl=1.13.12-00 2.2 master init # -------- master -------- *** init-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration imageRepository: registry.aliyuncs.com/google_containers kubernetesVersion: v1.16.1 controlPlaneEndpoint: \"192.168.1.37:6443\" networking: serviceSubnet: \"192.168.0.0/16\" podSubnet: \"192.168.0.0/16\" *** kubeadm config images pull --config=init-config.yaml kubeadm init --config=init-config.yaml # 不使用配置文件init kubeadm init \\ --apiserver-advertise-address=192.168.1.37 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.17.4 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 # kubeadm join 49.233.168.186:6443 --token cpnxfg.2as9a59xch4pt4a7 \\ # --discovery-token-ca-cert-hash sha256:33c8a4bcf84de94ae46ba33fb603e7366a58d6c4e8b6a71287f3f4040eaab33d mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 2.3 node init -------- node -------- sudo apt-get install -y kubelet kubeadm --disableexcludes=kubernetes *** join-config.yaml *** apiVersion: kubeadm.k8s.io/v1beta1 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: 49.233.168.186:6443 token: 4fbe4n.lkqwktq81wdi42ga unsafeSkipCAVerification: true tlsBootstrapToken: 4fbe4n.lkqwktq81wdi42ga *** kubeadm join --config=join-config.yaml 2.4 cni网络插件CNI网络插件选择参考:https://kubernetes.io/docs/setup/independent/createcluster-kubeadm/#pod-network Subnet选择192.168.0.0/16,使用calico kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml Subnet选择10.244.0.0/16,使用flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.5 gpu 插件 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml 2.6 单机 # 单机All-In-One Kubernetes环境 kubectl taint nodes --all node-role.kubernetes.io/master- 2.7 常用cli # pod状态 kubectl get pods --all-namespaces kubectl get pods --all-namespaces -o wide kubectl --namespace=kube-system describe pod 设置master节点参与/不参与Pod负载 master节点参与pod负载 kubectl taint nodes --all node-role.kubernetes.io/master- master节点恢复不参与POD负载 kubectl taint nodes node-role.kubernetes.io/master=:NoSchedule master节点恢复不参与POD负载，并将Node上已经存在的Pod驱逐出去 kubectl taint nodes node-role.kubernetes.io/master=:NoExecute node节点token失效 kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u003e/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' # 替换上面生成的相应字段 kubeadm join 10.167.11.153:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#21-k8s-download"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#22-master-init"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#23-node-init"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#24-cni网络插件"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#25-gpu-插件"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#26-单机"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#27-常用cli"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#设置master节点参与不参与pod负载"},{"categories":["CloudNative"],"content":" 2. k8s install by kubeadm 2.1 k8s download sudo swapoff -a sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat ","date":"2019-11-28","objectID":"/posts/kubernetes_install/:1:2","series":null,"tags":["Kubernetes"],"title":"Kubernetes安装","uri":"/posts/kubernetes_install/#node节点token失效"},{"categories":["CloudNative"],"content":" 参考 https://docs.docker.com/storage/storagedriver/ https://docs.docker.com/storage/storagedriver/device-mapper-driver/ Docker修改空间大小 Docker 环境 Storage Pool 用完解决方案：resize-device-mapper 关于docker动态扩展容器空间大小 dmsetup ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:0:1","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#参考"},{"categories":["CloudNative"],"content":" 1. Storage Driverstorage driver 可以让我们在 container 的可写层中创建数据,当删除 container后,这些数据不会保留;而且可写层的读写速度低于原生文件系统; ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:0","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#1-storage-driver"},{"categories":["CloudNative"],"content":" 1.1 images / layersimages由一系列的layer组成; images的每一层代表Dockerfile中的一条指令; 对运行中的container所做的所有更改都会写入 Thin R/W layer; ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:1","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#11-images--layers"},{"categories":["CloudNative"],"content":" 1.2 container / layers每个container都有自己的container layer, 并且所有更改都在container layer,所以多个容器可以共享对同一基础image的访问,但有自己的数据状态; Storage Driver 用来管理image layers 和 container layer; docker 提供了多种Storage driver, 每种Storage driver 对实现的处理方式不同,但是都使用可堆叠(stackable)的image layer和写时复制(CoW)策略; ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:2","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#12-container--layers"},{"categories":["CloudNative"],"content":" 1.3 container size on disk查看container size docker ps -s #docker pull node:14.0.0-stretch docker pull node:13.13.0-alpine3.11 #docker run -it -d --name node node:14.0.0-stretch docker run -it -d --name node node:13.13.0-alpine3.11 docker run -it -d --name node_v -v /home/deepbay/deepops/:/deepops node:14.0.0-stretch deep@test3:~$ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 4b7d70e701f1 node:14.0.0-stretch \"docker-entrypoint.s…\" 10 minutes ago Up 10 minutes node 0B (virtual 941MB) deep@test3:~$ docker exec -it node /bin/bash root@4b7d70e701f1:/# wget https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz --2020-04-27 06:18:24-- https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz Resolving dl.google.com (dl.google.com)... 203.208.40.39, 203.208.40.34, 203.208.40.41, ... Connecting to dl.google.com (dl.google.com)|203.208.40.39|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 123658438 (118M) [application/octet-stream] Saving to: 'go1.14.2.linux-amd64.tar.gz' go1.14.2.linux-amd64.tar.gz 100%[====================================================================================\u003e] 117.93M 10.2MB/s in 11s 2020-04-27 06:18:35 (10.7 MB/s) - 'go1.14.2.linux-amd64.tar.gz' saved [123658438/123658438] root@4b7d70e701f1:/# ls -alh | grep go -rw-r--r-- 1 root root 118M Apr 8 22:12 go1.14.2.linux-amd64.tar.gz root@4b7d70e701f1:/# exit exit deep@test3:~$ docker ps -s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZE 4b7d70e701f1 node:14.0.0-stretch \"docker-entrypoint.s…\" 14 minutes ago Up 14 minutes node 124MB (virtual 1.07GB) 在容器内下载golang安装包,docker container size 增加; size: container layer 的数据量 virtual size: image layers加上 container layer的数据量 ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:3","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#13-container-size-on-disk"},{"categories":["CloudNative"],"content":" 1.4 选择 storage driver https://docs.docker.com/storage/storagedriver/select-storage-driver/ overlay2 aufs devicemapper btrfs zfs vfs ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:1:4","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#14-选择-storage-driver"},{"categories":["CloudNative"],"content":" 2. Device Mapper","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:0","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#2-device-mapper"},{"categories":["CloudNative"],"content":" 2.1 设置storage driver为device mapper centos 7 sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum list docker-ce --showduplicates | sort -r sudo yum install docker-ce-18.03.1.ce docker-ce-cli-18.03.1.ce containerd.io sudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"storage-driver\": \"devicemapper\" }y EOF sudo systemctl daemon-reload sudo systemctl restart docker # start up automatically on boot up sudo systemctl enable docker \u0026\u0026 sudo systemctl start docker sudo groupadd docker sudo gpasswd -a sunyh docker sudo service docker restart reboot ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:1","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#21-设置storage-driver为device-mapper"},{"categories":["CloudNative"],"content":" 2.2 设置 docker Pool Data Space DATA_SIZE=1000 METADATA_SIZE=10 sudo systemctl stop docker dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/data bs=1G count=0 seek=$DATA_SIZE dd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/metadata bs=1G count=0 seek=$METADATA_SIZE sudo systemctl start docker ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:2","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#22-设置-docker-pool-data-space"},{"categories":["CloudNative"],"content":" 2.3 设置 docker device size (container size) docker pull node:13.13.0-alpine3.11 docker run -it -d --name node node:13.13.0-alpine3.11 docker ps -s dmsetup table resize container echo $((30*1024*1024*1024/512)) echo 0 41943040 thin 253:0 7 | sudo dmsetup load docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a sudo dmsetup resume docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a sudo xfs_growfs -d /dev/mapper/docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 脚本简化以上流程: 获取deviceName # docker inspect -f '{{ .GraphDriver.Data.DeviceName }}' [container id] docker inspect -f '{{ .GraphDriver.Data.DeviceName }}' 2d9ddb14b348 使用dmsetup table 命令显示出device mapper的具体信息 # dmsetup table [device name] sudo dmsetup table docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 当容器在stop状态时以上操作得不到device mapper信息，虽然由于空 间不足不能启动成功，但需要先docker start 才能查找到对应 device mapper信息。 算出想要扩容的扇区数 SIZE=15 echo $(($SIZE*1024*1024*1024/512)) load一个新的设备信息表(新表与旧表相比只修改扇区数量) echo 0 31457280 thin 253:0 7 | sudo dmsetup load docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 通过dmsetup resume激活新的设备信息表 sudo dmsetup resume docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a 调整文件系统大小(只能增,不能减) File Type Resize CLI xfs xfs_growfs ex2,ex3,ex4 resize2fs sudo xfs_growfs -d /dev/mapper/docker-8:2-101407194-f69fd6ac662db4fd50e56ad2d4d45987bf31ade7b0b17a2e9d29b333df98c76a import paramiko hostname = '192.168.232.133' username = 'sunyh' password = 'hisunyh' container_id = \"3afd866418592636fc27754bfebdf29576daa5c22118e323a912320571dcabc0\" device_size = 16 paramiko.util.log_to_file('syslogin.log') # 发送paramiko日志到syslogin.log文件 def resize_container(): ssh = paramiko.SSHClient() # 获取客户端host_keys,默认~/.ssh/known_hosts,非默认路径需指定ssh.load_system_host_keys(/xxx/xxx) ssh.load_system_host_keys() ssh.connect(hostname=hostname, username=username, password=password) # echo \"yourpasswd\" |sudo -S yourcommand stdin, stdout, stderr = ssh.exec_command(\"docker inspect -f '{{ .GraphDriver.Data.DeviceName }}' %s\" % container_id) # print(stdout.read().decode('utf-8')) device_name = stdout.readlines()[0] if device_name.find('docker') == -1: print('get device name failed') return print('device name: %s' % device_name) stdin, stdout, stderr = ssh.exec_command(\"echo %s | sudo -S dmsetup table %s\" % (password, device_name)) device_mapper = stdout.readlines()[0] if device_mapper.find('thin') == -1: print('get device mapper info failed') return print('device mapper info: %s' % device_mapper) device_sector = device_mapper.replace('\\n', '').split('thin')[1] print('device sector: %s' % device_sector) stdin, stdout, stderr = ssh.exec_command('echo 0 %s thin%s | sudo dmsetup load %s' % (device_size * 1024 * 1024 * 1024 / 512, device_sector, device_name)) print('dmsetup load finish') stdin, stdout, stderr = ssh.exec_command('sudo dmsetup resume %s' % device_name) print('dmsetup resume finish') fs_line = 'sudo xfs_growfs -d /dev/mapper/%s' % device_name print(fs_line) stdin, stdout, stderr = ssh.exec_command(fs_line) print(stdout.readlines()) print('file system modify finish') ssh.close() pass if __name__ == \"__main__\": resize_container() ","date":"2019-11-25","objectID":"/posts/docker_storage_driver/:2:3","series":null,"tags":["Docker"],"title":"Docker Storage Driver","uri":"/posts/docker_storage_driver/#23-设置-docker-device-size-container-size"},{"categories":["CloudNative"],"content":"Kafka, MongoDB ","date":"2019-11-20","objectID":"/posts/docker_examples/:0:0","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#"},{"categories":["CloudNative"],"content":" Kafka docker pull wurstmeister/zookeeper docker pull wurstmeister/kafka docker run -d --name zookeeper -p 2181:2181 -t wurstmeister/zookeeper docker run -d --name kafka --publish 9092:9092 \\ --link zookeeper \\ --env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\ --env KAFKA_ADVERTISED_HOST_NAME=127.0.0.1 \\ --env KAFKA_ADVERTISED_PORT=9092 \\ wurstmeister/kafka test # smoke test docker exec -it kafka /bin/bash $ /opt/kafka/bin/kafka-console-producer.sh --topic=test --broker-list localhost:9092 $ /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 -from-beginning --topic test ","date":"2019-11-20","objectID":"/posts/docker_examples/:1:0","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#kafka"},{"categories":["CloudNative"],"content":" MongoDB","date":"2019-11-20","objectID":"/posts/docker_examples/:2:0","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#mongodb"},{"categories":["CloudNative"],"content":" 1. install mongodb by docker 转自 https://blog.csdn.net/weixin_44591832/article/details/91953189 # docker pull mongo:4.2.5 docker pull mongo:3.6.17 # docker run --name mongodb -p 27017:27017 -d mongo:4.2.5 --auth docker run --name mongodb -p 27017:27017 -d mongo:3.6.17 MongoDB添加管理员 docker exec -it mongodb mongo admin 创建admini管理员账号: # 在所有数据库管理用户 db.createUser({ user: 'root', pwd: 'root', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] }); # 授权管理所有数据库 db.createUser({ user: 'admin', pwd: 'admin123', roles: [ { role: \"dbAdminAnyDatabase\", db: \"admin\" } ] }); $ docker exec -it mongodb mongo admin MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/admin?compressors=disabled\u0026gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"685f78a0-4730-4192-a5c5-39677312e765\") } MongoDB server version: 4.2.5 Welcome to the MongoDB shell. For interactive help, type \"help\". For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user \u003e db.createUser({ user: 'root', pwd: 'root', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] }); Successfully added user: { \"user\" : \"root\", \"roles\" : [ { \"role\" : \"userAdminAnyDatabase\", \"db\" : \"admin\" } ] } \u003e exit bye 创建普通用户: docker exec -it mongodb mongo admin db.auth(\"root\",\"root\"); db.createUser({ user: 'sunyh', pwd: 'hi123456', roles: [ { role: \"readWrite\", db: \"ngc\" } ] }); db.createUser({ user: 'sunyh', pwd: 'hi123456', roles: [ { role: \"dbOwner\", db: \"ngc\" } ] }); $ docker exec -it mongodb mongo admin MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/admin?compressors=disabled\u0026gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"6658ca1e-15e5-40e4-a507-8a9fab70bee1\") } MongoDB server version: 4.2.5 \u003e db.auth(\"root\",\"root\"); 1 \u003e db.createUser({ user: 'sunyh', pwd: 'hi123456', roles: [ { role: \"dbOwner\", db: \"ngc\" } ] }); Successfully added user: { \"user\" : \"sunyh\", \"roles\" : [ { \"role\" : \"dbOwner\", \"db\" : \"app\" } ] } \u003e exit bye db.auth(\"sunyh\",\"hi123456\"); use ngc db.test.save({name:\"hello ngc\"}); $ docker exec -it mongodb mongo admin MongoDB shell version v4.2.5 connecting to: mongodb://127.0.0.1:27017/admin?compressors=disabled\u0026gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"19c22492-6b5b-4a99-bbcc-a2c852d003cb\") } MongoDB server version: 4.2.5 \u003e db.auth(\"sunyh\",\"hi123456\"); 1 \u003e use ngc switched to db ngc \u003e db.test.save({name:\"hello ngc\"}); WriteResult({ \"nInserted\" : 1 }) \u003e exit bye ","date":"2019-11-20","objectID":"/posts/docker_examples/:2:1","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#1-install-mongodb-by-docker"},{"categories":["CloudNative"],"content":" 2. mongo express https://github.com/mongo-express/mongo-express-docker docker pull mongo-express:0.54 docker run -it -d --name mongo-express --link mongodb:mongo -p 28081:8081 mongo-express:0.54 docker run -it --rm \\ --name mongo-express \\ --link mongodb:mongo \\ -p 8081:8081 \\ -e ME_CONFIG_OPTIONS_EDITORTHEME=\"ambiance\" \\ -e ME_CONFIG_BASICAUTH_USERNAME=\"user\" \\ -e ME_CONFIG_BASICAUTH_PASSWORD=\"fairly long password\" \\ mongo-express docker stop mongo-express docker rm mongo-express ","date":"2019-11-20","objectID":"/posts/docker_examples/:2:2","series":null,"tags":["Docker"],"title":"Docker常用应用示例","uri":"/posts/docker_examples/#2-mongo-express"},{"categories":["CloudNative"],"content":" 下载安装 Environment: Ubuntu18.04 Reference: https://docs.docker.com/engine/install/ubuntu/ # 卸载 docker sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # docker version list apt-cache madison docker-ce apt-cache madison docker-ce-cli # install sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # 安装指定版本 sudo apt-get install docker-ce=5:18.09.9~3-0~ubuntu-bionic docker-ce-cli=5:18.09.9~3-0~ubuntu-bionic containerd.io # 设置 daemon.json sudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://registry.docker-cn.com\"] } EOF # reboot docker sudo systemctl daemon-reload sudo systemctl restart docker # start up automatically on boot up sudo systemctl enable docker \u0026\u0026 sudo systemctl start docker # docker permission deny sudo groupadd docker sudo gpasswd -a [user name] docker sudo service docker restart ","date":"2019-11-20","objectID":"/posts/docker_basics/:1:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#下载安装"},{"categories":["CloudNative"],"content":" 修改存储位置 // 修改docker默认镜像/容器存储位置 // Docker默认的镜像和容器存储位置在/var/lib/docke 1)修改docker.service文件　cd /etc/systemd/system/multi-user.target.wants sudo vim docker.service // ExecStart=/usr/bin/dockerd --graph=/data/docker --storage-driver=overlay --graph=/data/docker：docker新的存储位置 --storage-driver=overlay ： 当前docker所使用的存储驱动 2) 重启docker　sudo systemctl daemon-reload sudo systemctl restart docker ","date":"2019-11-20","objectID":"/posts/docker_basics/:2:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#修改存储位置"},{"categories":["CloudNative"],"content":" Docker常用操作 # docker --help build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes # docker build --help Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Run a command in a new container Options: --add-host list Add a custom host-to-IP mapping (host:ip) -d, --detach Run container in background and print container ID -e, --env list Set environment variables --gpus gpu-request GPU devices to add to the container ('all' to pass all GPUs) --help Print usage -h, --hostname string Container host name -i, --interactive Keep STDIN open even if not attached -m, --memory bytes Memory limit --mount mount Attach a filesystem mount to the container --name string Assign a name to the container --network network Connect a container to a network --privileged Give extended privileges to this container -p, --publish list Publish a container's port(s) to the host --rm Automatically remove the container when it exits --shm-size bytes Size of /dev/shm -t, --tty Allocate a pseudo-TTY --ulimit ulimit Ulimit options (default []) -u, --user string Username or UID (format: \u003cname|uid\u003e[:\u003cgroup|gid\u003e]) -v, --volume list Bind mount a volume -w, --workdir string Working directory inside the container Examples # list all containers docker container ls -a / docker ps -a # list all containers id docker container ls -aq / docker contaier ls -a | awk {'print$1'} # remove all containers docker rm $(docker container ls -aq) # list all exited containers docker container ls -f \"status=exited\" # list all exited containers id docker container ls -f \"status=exited\" -q docker ps -a|grep Exited|awk '{print $1}' # remove all exited containers docker rm $(docker container ls -f \"status=exited\" -q) docker rm `docker ps -a|grep Exited|awk '{print $1}'` docker rm $(docker ps -qf status=exited) #删除名称或标签为none的镜像 docker rmi -f `docker images | grep '\u003cnone\u003e' | awk '{print $3}'` #删除所有名字中带 “dee” 关键字的镜像 docker rmi $(docker images | grep \"dee\" | awk '{print $3}') # docker commit [CONTAINER] [REPOSITORY] docker run -d -it --name nginx nginx docker commit nginx hisunyh/my-nginx:1.0 docker run -d -it --name crawler --add-host master.namenode:192.168.1.25 --add-host slave.datanode1:192.168.1.26 --add-host slave.datanode2:192.168.1.29 -e \"que","date":"2019-11-20","objectID":"/posts/docker_basics/:3:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#docker常用操作"},{"categories":["CloudNative"],"content":" Dockerfile Refenence: https://docs.docker.com/engine/reference/builder/ https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ Samples sudo tee ./Dockerfile.dev \u003c\u003c-'EOF' FROM golang:1.14-alpine AS prod-build ENV GOPROXY https://goproxy.cn,direct # ENV GO111MODULE on WORKDIR /workspace COPY . /workspace RUN CGO_ENABLED=0 GOOS=linux go build -o deep_arena . # FROM scratch AS prod-final FROM debian:9.12 AS prod-final RUN rm -f /etc/localtime \\ \u0026\u0026 ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ \u0026\u0026 echo \"Asia/Shanghai\" \u003e /etc/timezone COPY --from=prod-build /workspace/deep_arena . COPY --from=prod-build /workspace/conf.toml . COPY --from=prod-build /workspace/config . EXPOSE 8082 CMD [\"/deep_arena\"] EOF docker login my-horbor.com docker build -f ./Dockerfile.dev -t my-horbor.com/deep_arena:dev_v1.0 . docker push my-horbor.com/deep_arena:dev_v1.0 docker pull my-horbor.com/deep_arena:dev_v1.0 docker run -d -it -p 9082:8082 --name da -v ~/pod_logs:/pod_logs my-horbor.com/deep_arena:dev_v1.0 ","date":"2019-11-20","objectID":"/posts/docker_basics/:4:0","series":null,"tags":["Docker"],"title":"Docker基础","uri":"/posts/docker_basics/#dockerfile"},{"categories":["FS"],"content":" curl -k -u admin:admin001 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"fset007\", \"path\": \"/gpfs/fset007\", \"owner\": \"sunyh\", \"permissions\": \"700\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' curl -k -u admin:admin001 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"fset007\", \"path\": \"/gpfs/fset007\", \"owner\": \"sunyh\", \"permissions\": \"700\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' curl -k -u admin:admin001 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"fset007\", \"path\": \"/gpfs/fset007\", \"owner\": \"sunyh\", \"permissions\": \"700\", \"inodeSpace\": \"root\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' curl -k -u admin:admin001 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/jobs/1000000000013' ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:0","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#"},{"categories":["FS"],"content":" 创建文件集 curl -k -u admin001:deepbay2010 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{ \"filesetName\": \"feitest\", \"owner\" : \"root\", \"path\": \"/gpfs/feitest\", \"permissions\": \"777\" }' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' 注释： filesetName ：文件集名称 owner： 新文件集所有者的用户ID，然后可选地加上组ID，例如“ root：”。 path：文件集路径 permissions：权限， 相当于使用chmod设置的权限 tip：创建文件集的时候，path不能存在，否则会创建失败。 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:1","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#创建文件集"},{"categories":["FS"],"content":" 设置文件集配额 curl -k -u admin001:deepbay2010 -X POST --header 'content-type:application/json' --header 'accept:application/json' -d '{\"operationType\": \"setQuota\",\"quotaType\": \"FILESET\",\"objectName\": \"feitest\",\"blockSoftLimit\": \"1G\",\"blockHardLimit\": \"1G\"}' \"https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/quotas\" 注释： objectName：文件集名称 blockSoftLimit：软限制 （警告） blockHardLimit：硬限制 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:2","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#设置文件集配额"},{"categories":["FS"],"content":" 删除文件集 curl -k -u admin001:deepbay2010 -X DELETE --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets/{feilSetName}' 注释： 路径最后为文件集名称 tip：删除文件集会把该文件集对应路径下的所有文件删除 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:3","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#删除文件集"},{"categories":["FS"],"content":" 获取指定文件集信息 curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets/{feilSetName}' ​ ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:4","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取指定文件集信息"},{"categories":["FS"],"content":" 获取简略的所有文件集信息 curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets' ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:5","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取简略的所有文件集信息"},{"categories":["FS"],"content":" 获取指定文件集配额信息 curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets/{feilSetName}/quotas' ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:6","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取指定文件集配额信息"},{"categories":["FS"],"content":" 获取配额使用信息 (所有的文件集) curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/quotas' tip：等效于 mmrepquota -g -v -a ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:7","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#获取配额使用信息----所有的文件集"},{"categories":["FS"],"content":" 注意事项 上传文件超过配额大小时，会中止上传，但是已上传的部分会留在磁盘内，建议删除已上传的部分。 不能使用root用户进行文件上传，否则上传配额失效。 gui页面 https://192.168.1.50 官网 https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_apiv2postfilesystemfilesets.htm 版本：4.2.3 ，V2 ","date":"2019-11-20","objectID":"/posts/gpfs_api/:0:8","series":null,"tags":["GPFS"],"title":"GPFS API文档","uri":"/posts/gpfs_api/#注意事项"},{"categories":["FS"],"content":" GPFS安装配置","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#gpfs安装配置"},{"categories":["FS"],"content":" 1. 操作系统配置 SELINUX 修改/etc/selinux/config 文件将 selinux 设置为 disable。 防火墙 使用 systemctl stop firewalld 来关闭防火墙，使用 systemctl disable firewalld 来 禁用防火墙自动启动服务。 软件包 将软件安装所需的依赖软件包件复制到了/root/rhel72/目录中 # scp -r rhel72 root@192.168.1.48:/root 网络配置 更新服务器/etc/hosts # gpfs 192.168.1.50 gpfs-io-1 192.168.1.48 gpfs-io-2 192.168.1.162 gpfs-cli-1 192.168.1.33 gpfs-cli-2 192.168.1.29 gpfs-cli-3 192.168.1.30 gpfs-cli-4 ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:1","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#1-操作系统配置"},{"categories":["FS"],"content":" 2. I/O 服务器端安装准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 # 每个节点执行 # 免密 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-2 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-2 2.1 软件安装安装 gpfs rpm 包 # 基础包 # -ivh cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -ivh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # 安装升级包 # -Uvh # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # --force # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh --force gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # -------------------- # ----- Ubuntu ----- # cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb # 卸载 # dpkg -l | grep gpfs # dpkg -P gpfs.base gpfs.docs gpfs.ext gpfs.gpl gpfs.gskit gpfs.gss.pmcollector gpfs.gss.pmsensors gpfs.java gpfs.msg.en-us 编译 cd /usr/lpp/mmfs/src \u0026\u0026 make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX \u0026\u0026 make World \u0026\u0026 make InstallImages 如果编译提示没有kernel-headers,安装对应的版本的kernel-headers # yum install -y kernel-headers kernel-devel cd ~/rhel72/centos7.7-kernel/ \u0026\u0026 rpm -ivh kernel-headers-3.10.0-1062.el7.x86_64.rpm kernel-devel-3.10.0-1062.el7.x86_64.rpm yum install -y gcc gcc-c++ 将gpfs执行文件路径添加到环境变量 echo 'PATH=$PATH:/usr/lpp/mmfs/bin' \u003e\u003e /etc/profile \u0026\u0026 source /etc/profile 删除已有集群节点: # https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1pdg_nodnoad.htm mmdelnode -f 2.2 初始化配置文件 创建存储集群 只需在 gpfs-io-1 配置 创建存储集群需要两个配置文件，一个用于设定服务器的角色(mmcrcluster-node.lst) ， 一 个 用 于 设 定 存 储 集 群 的 参 数 配 置(mmcrcluster-config.lst)，两个文件的内容参考如下： tee ~/mmcrcluster-node.lst \u003c\u003c-'EOF' gpfs-io-1:manager-quorum gpfs-io-2:manager-quorum EOF tee ~/mmcrcluster-config.lst \u003c\u003c-'EOF' pagepool 4096M maxMBpS 22400 maxblocksize 16m EOF 创建集群: mmcrcluster -N mmcrcluster-node.lst -p gpfs-io-1 -s gpfs-io-2 -r /usr/bin/ssh -R /usr/bin/scp -C GridScaler-Cluster -A -c mmcrcluster-config.lst 为节点添加License: mmchlicense server --accept -N gpfs-io-1,gpfs-io-2 .创建 NSD 磁盘 Multipath配置 # http://www.linuxboy.net/linuxjc/144375.html yum install -y device-mapper-multipath mpathconf --enable lsmod |grep dm_multipath modprobe dm-multipath modprobe dm-round-robin service multipathd start multipath –v2 multipath -ll 如果multipath -ll没输出,编辑配置文件/etc/multipath.conf,将find_multipaths yes 注释 vi /etc/multipath.conf 重启,然后验证是否配置成功 准备并行文件系统的磁盘检测脚本 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices编辑/var/mmfs/etc/nsddevices 文件，如下图所示增加/dev/mapper 一行内容。 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices \u0026\u0026 chmod +777 /var/mmfs/etc/nsddevices \u0026\u0026 vi /var/mmfs/etc/nsddevices # ls -l /dev/mapper/ 2\u003e/dev/null | awk '{print \"mapper/\"$9 \" generic\"}' 重启每一台服务器 init 6 编写 NSD 磁盘脚本 tee ~/nsd.conf \u003c\u003c-'EOF' %nsd: nsd=NSD_1 device=/dev/mapper/mpathb servers=gpfs-io-1,gpfs-io-2 usage=dataAndMetadata failureGroup=100 pool=system EOF servers 为该 LUN 被访问的 I/O 服务器顺序，当 gpfs-io-1 服务器关机或宕机时，gpfs-io-2 优先取得该 LUN 的控制权. 创建 NSD 磁盘 # 创建 NSD 磁盘 mmcrnsd -F nsd.conf # 删除 NSD 磁盘 # mmdelnsd -F nsd.conf 创建磁盘过程出现错误: refers to an existing NSD, 加上 -v no: 参考:https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_mmcrnsd.htm mmcrnsd -F -v no nsd.conf 创建文件系统 # 创建文件系统 mmcrfs gpfs -F nsd.conf -m 1 -M 2 -r 1 -R 2 -A yes -B 16m -n 512 -j cluster -Q yes -T /gpfs # 查看文件系统 mmlsfs all 上述命令中 gpfs 为文件系统名称，-A yes 代表文件系统自动 mount，-B 16m 代表文件系统块大小为 16m，-n 512 代表该集群最大支持节点数，-j","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#2-io-服务器端安装"},{"categories":["FS"],"content":" 2. I/O 服务器端安装准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 # 每个节点执行 # 免密 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-2 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-2 2.1 软件安装安装 gpfs rpm 包 # 基础包 # -ivh cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -ivh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # 安装升级包 # -Uvh # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # --force # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh --force gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # -------------------- # ----- Ubuntu ----- # cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb # 卸载 # dpkg -l | grep gpfs # dpkg -P gpfs.base gpfs.docs gpfs.ext gpfs.gpl gpfs.gskit gpfs.gss.pmcollector gpfs.gss.pmsensors gpfs.java gpfs.msg.en-us 编译 cd /usr/lpp/mmfs/src \u0026\u0026 make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX \u0026\u0026 make World \u0026\u0026 make InstallImages 如果编译提示没有kernel-headers,安装对应的版本的kernel-headers # yum install -y kernel-headers kernel-devel cd ~/rhel72/centos7.7-kernel/ \u0026\u0026 rpm -ivh kernel-headers-3.10.0-1062.el7.x86_64.rpm kernel-devel-3.10.0-1062.el7.x86_64.rpm yum install -y gcc gcc-c++ 将gpfs执行文件路径添加到环境变量 echo 'PATH=$PATH:/usr/lpp/mmfs/bin' \u003e\u003e /etc/profile \u0026\u0026 source /etc/profile 删除已有集群节点: # https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1pdg_nodnoad.htm mmdelnode -f 2.2 初始化配置文件 创建存储集群 只需在 gpfs-io-1 配置 创建存储集群需要两个配置文件，一个用于设定服务器的角色(mmcrcluster-node.lst) ， 一 个 用 于 设 定 存 储 集 群 的 参 数 配 置(mmcrcluster-config.lst)，两个文件的内容参考如下： tee ~/mmcrcluster-node.lst \u003c\u003c-'EOF' gpfs-io-1:manager-quorum gpfs-io-2:manager-quorum EOF tee ~/mmcrcluster-config.lst \u003c\u003c-'EOF' pagepool 4096M maxMBpS 22400 maxblocksize 16m EOF 创建集群: mmcrcluster -N mmcrcluster-node.lst -p gpfs-io-1 -s gpfs-io-2 -r /usr/bin/ssh -R /usr/bin/scp -C GridScaler-Cluster -A -c mmcrcluster-config.lst 为节点添加License: mmchlicense server --accept -N gpfs-io-1,gpfs-io-2 .创建 NSD 磁盘 Multipath配置 # http://www.linuxboy.net/linuxjc/144375.html yum install -y device-mapper-multipath mpathconf --enable lsmod |grep dm_multipath modprobe dm-multipath modprobe dm-round-robin service multipathd start multipath –v2 multipath -ll 如果multipath -ll没输出,编辑配置文件/etc/multipath.conf,将find_multipaths yes 注释 vi /etc/multipath.conf 重启,然后验证是否配置成功 准备并行文件系统的磁盘检测脚本 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices编辑/var/mmfs/etc/nsddevices 文件，如下图所示增加/dev/mapper 一行内容。 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices \u0026\u0026 chmod +777 /var/mmfs/etc/nsddevices \u0026\u0026 vi /var/mmfs/etc/nsddevices # ls -l /dev/mapper/ 2\u003e/dev/null | awk '{print \"mapper/\"$9 \" generic\"}' 重启每一台服务器 init 6 编写 NSD 磁盘脚本 tee ~/nsd.conf \u003c\u003c-'EOF' %nsd: nsd=NSD_1 device=/dev/mapper/mpathb servers=gpfs-io-1,gpfs-io-2 usage=dataAndMetadata failureGroup=100 pool=system EOF servers 为该 LUN 被访问的 I/O 服务器顺序，当 gpfs-io-1 服务器关机或宕机时，gpfs-io-2 优先取得该 LUN 的控制权. 创建 NSD 磁盘 # 创建 NSD 磁盘 mmcrnsd -F nsd.conf # 删除 NSD 磁盘 # mmdelnsd -F nsd.conf 创建磁盘过程出现错误: refers to an existing NSD, 加上 -v no: 参考:https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_mmcrnsd.htm mmcrnsd -F -v no nsd.conf 创建文件系统 # 创建文件系统 mmcrfs gpfs -F nsd.conf -m 1 -M 2 -r 1 -R 2 -A yes -B 16m -n 512 -j cluster -Q yes -T /gpfs # 查看文件系统 mmlsfs all 上述命令中 gpfs 为文件系统名称，-A yes 代表文件系统自动 mount，-B 16m 代表文件系统块大小为 16m，-n 512 代表该集群最大支持节点数，-j","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#21-软件安装"},{"categories":["FS"],"content":" 2. I/O 服务器端安装准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 # 每个节点执行 # 免密 ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-io-2 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-1 ssh-copy-id -i ~/.ssh/id_rsa.pub gpfs-cli-2 2.1 软件安装安装 gpfs rpm 包 # 基础包 # -ivh cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -ivh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # 安装升级包 # -Uvh # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # --force # cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh --force gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # -------------------- # ----- Ubuntu ----- # cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb # 卸载 # dpkg -l | grep gpfs # dpkg -P gpfs.base gpfs.docs gpfs.ext gpfs.gpl gpfs.gskit gpfs.gss.pmcollector gpfs.gss.pmsensors gpfs.java gpfs.msg.en-us 编译 cd /usr/lpp/mmfs/src \u0026\u0026 make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX \u0026\u0026 make World \u0026\u0026 make InstallImages 如果编译提示没有kernel-headers,安装对应的版本的kernel-headers # yum install -y kernel-headers kernel-devel cd ~/rhel72/centos7.7-kernel/ \u0026\u0026 rpm -ivh kernel-headers-3.10.0-1062.el7.x86_64.rpm kernel-devel-3.10.0-1062.el7.x86_64.rpm yum install -y gcc gcc-c++ 将gpfs执行文件路径添加到环境变量 echo 'PATH=$PATH:/usr/lpp/mmfs/bin' \u003e\u003e /etc/profile \u0026\u0026 source /etc/profile 删除已有集群节点: # https://www.ibm.com/support/knowledgecenter/STXKQY_5.0.0/com.ibm.spectrum.scale.v5r00.doc/bl1pdg_nodnoad.htm mmdelnode -f 2.2 初始化配置文件 创建存储集群 只需在 gpfs-io-1 配置 创建存储集群需要两个配置文件，一个用于设定服务器的角色(mmcrcluster-node.lst) ， 一 个 用 于 设 定 存 储 集 群 的 参 数 配 置(mmcrcluster-config.lst)，两个文件的内容参考如下： tee ~/mmcrcluster-node.lst \u003c\u003c-'EOF' gpfs-io-1:manager-quorum gpfs-io-2:manager-quorum EOF tee ~/mmcrcluster-config.lst \u003c\u003c-'EOF' pagepool 4096M maxMBpS 22400 maxblocksize 16m EOF 创建集群: mmcrcluster -N mmcrcluster-node.lst -p gpfs-io-1 -s gpfs-io-2 -r /usr/bin/ssh -R /usr/bin/scp -C GridScaler-Cluster -A -c mmcrcluster-config.lst 为节点添加License: mmchlicense server --accept -N gpfs-io-1,gpfs-io-2 .创建 NSD 磁盘 Multipath配置 # http://www.linuxboy.net/linuxjc/144375.html yum install -y device-mapper-multipath mpathconf --enable lsmod |grep dm_multipath modprobe dm-multipath modprobe dm-round-robin service multipathd start multipath –v2 multipath -ll 如果multipath -ll没输出,编辑配置文件/etc/multipath.conf,将find_multipaths yes 注释 vi /etc/multipath.conf 重启,然后验证是否配置成功 准备并行文件系统的磁盘检测脚本 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices编辑/var/mmfs/etc/nsddevices 文件，如下图所示增加/dev/mapper 一行内容。 cp /usr/lpp/mmfs/samples/nsddevices.sample /var/mmfs/etc/nsddevices \u0026\u0026 chmod +777 /var/mmfs/etc/nsddevices \u0026\u0026 vi /var/mmfs/etc/nsddevices # ls -l /dev/mapper/ 2\u003e/dev/null | awk '{print \"mapper/\"$9 \" generic\"}' 重启每一台服务器 init 6 编写 NSD 磁盘脚本 tee ~/nsd.conf \u003c\u003c-'EOF' %nsd: nsd=NSD_1 device=/dev/mapper/mpathb servers=gpfs-io-1,gpfs-io-2 usage=dataAndMetadata failureGroup=100 pool=system EOF servers 为该 LUN 被访问的 I/O 服务器顺序，当 gpfs-io-1 服务器关机或宕机时，gpfs-io-2 优先取得该 LUN 的控制权. 创建 NSD 磁盘 # 创建 NSD 磁盘 mmcrnsd -F nsd.conf # 删除 NSD 磁盘 # mmdelnsd -F nsd.conf 创建磁盘过程出现错误: refers to an existing NSD, 加上 -v no: 参考:https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_mmcrnsd.htm mmcrnsd -F -v no nsd.conf 创建文件系统 # 创建文件系统 mmcrfs gpfs -F nsd.conf -m 1 -M 2 -r 1 -R 2 -A yes -B 16m -n 512 -j cluster -Q yes -T /gpfs # 查看文件系统 mmlsfs all 上述命令中 gpfs 为文件系统名称，-A yes 代表文件系统自动 mount，-B 16m 代表文件系统块大小为 16m，-n 512 代表该集群最大支持节点数，-j","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#22-初始化配置文件"},{"categories":["FS"],"content":" 3. Linux 计算节点客户端安装准备: I/O 节点之间以及 I/O 节点与计算节点之间需要设定好 ssh 无密码访问功能。 # io node ssh-keygen -t rsa ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22 root@192.168.1.29 ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22 root@192.168.1.30 I/O 节点与计算节点的主机名以及 IP 地址能够正确解析。 关闭计算节点的防火墙。 设置网络 软件安装: # centos cd ~/rhel72/4.2.3.22/ \u0026\u0026 rpm -Uvh gpfs.base-4.2.3-22.x86_64.rpm gpfs.docs-4.2.3-22.noarch.rpm gpfs.gpl-4.2.3-22.noarch.rpm gpfs.gskit-8.0.50-86.x86_64.rpm gpfs.msg.en_US-4.2.3-22.noarch.rpm gpfs.ext-4.2.3-22.x86_64.rpm # ubuntu cd ~/rhel72/4.2.3.22/ \u0026\u0026 dpkg -i gpfs.base_4.2.3-22_amd64.deb gpfs.docs_4.2.3-22_all.deb gpfs.gpl_4.2.3-22_all.deb gpfs.gskit_8.0.50-86_amd64.deb gpfs.msg.en-us_4.2.3-22_all.deb gpfs.ext_4.2.3-22_amd64.deb 编译: cd /usr/lpp/mmfs/src make Autoconfig LINUX_DISTRIBUTION=REDHAT_AS_LINUX make World make InstallImages 客户端加到gpfs: mmaddnode -N gpfs-cli-1 mmaddnode -N gpfs-cli-2 mmaddnode -N gpfs-cli-3 mmaddnode -N gpfs-cli-4 # 删除节点 # mmdelnode -N gpfs-cli-1 # 删除全部节点 # mmdelnode -a # 节点与cluster失联,可以在node节点删除 # mmdelnode -f # if addnode error sudo apt-get autoremove sudo apt --fix-broken install 客户端添加License: mmchlicense client --accept -N gpfs-cli-1 mmchlicense client --accept -N gpfs-cli-2 mmchlicense client --accept -N gpfs-cli-3 mmchlicense client --accept -N gpfs-cli-4 ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:1:3","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#3-linux-计算节点客户端安装"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#gpfs-常用cli"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统服务启动"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统服务的停止"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统服务状态"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#手工-mountumount-文件系统"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看文件系统-mount-状态"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统集群信息"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统集群配置信息"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统-gpfs-的配置信息"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统集群的磁盘信息"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#查看并行文件系统对应的磁盘信息"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统重要配置文件"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#并行文件系统的日志文件"},{"categories":["FS"],"content":" GPFS 常用CLI 并行文件系统服务启动mmstartup，启动当前服务器的并行文件系统服务； mmstartup -N node01，启动服务器 node01 的并行文件系统服务； mmstartup -a，启动所有并行文件系统集群列表中的服务器的并行文件系统服务； 默认并行文件系统的服务进程是随着操作系统启动时自动启动的； 并行文件系统服务的停止mmshutdown，停止当前服务器的并行文件系统服务； mmshutdown -N node01，停止服务器 node01 的并行文件系统服务； mmshutdown -a，停止所有并行文件系统集群列表中的服务器的并行文件系统服务； 查看并行文件系统服务状态mmgetstate，查看当前服务器的并行文件系统服务状态； mmgetstate -N node01，查看服务器 node01 的并行文件系统服务状态； mmgetstate -a，查看所有并行文件系统集群列表中的服务器的并行文件系统服务状态； 通常的状态有 active（正常运行状态），down（服务停止状态），arbitrating（服务已启动，但是正在寻找主管理节点） 手工 mount/umount 文件系统！通常情况下，并行文件系统服务启动后，文件系统会自动挂载，出于维护或者其他原因，我们可能需要手工 umount 或者手工 mount 文件系统。 mmmount gpfs，在当前服务器上 mount 文件系统； mmumount gpfs，在当前服务器上 umount 文件系统； mmmount gpfs -N node01，在 node01 服务器上 mount 文件系统； mmumount gpfs -N node01，在 node01 服务器上 umount 文件系统； mmmount gpfs -a，在并行文件系统集群中的所有节点上 mount 文件系统； mmumount gpfs -a，在并行文件系统集群中的所有节点上 umount 文件系统; 查看文件系统 mount 状态mmlsmount gpfs -L 查看并列出 gpfs 文件系统在哪些节点 mount 了； 查看并行文件系统集群信息mmlscluster 查看并行文件系统集群配置信息mmlsconfig 查看并行文件系统 gpfs 的配置信息mmlsfs gpfs 查看并行文件系统集群的磁盘信息mmlsnsd -M 查看并行文件系统对应的磁盘信息mmlsdisk gpfs -M 并行文件系统重要配置文件/var/mmfs/gen/mmsdrfs 并行文件系统的日志文件当前日志文件/var/mmfs/gen/mmfslog 所有日志文件/var/adm/ras/目录 设置磁盘配额首先要确认已经创建好的 gpfs 并行文件系统已经打开了磁盘配额功能。 mmlsfs gpfs (此处的 gpfs 是文件系统的名称，可以通过 mmlsconfig 命令来查看文件系统的名称-命令输出的最后一行)。 mmlsfs 输出的-Q 对应的设置如果是 yes，则说明文件系统已经打开了磁盘配额功能。如果是 no，则说明文件系统的配额功能没有打开。可以通过如下命令来打开配额功能：mmchfs gpfs -Q=yes GPFS 的磁盘配额设置方法具体如下： 1．设置默认用户磁盘配额： mmdefquotaon -d gpfs 激活默认 quota a)mmdefedquota -u gpfs，在出现的界面中修改相应的容量限制数值。修改完毕后wq 保存退出！ b)mmdefquotaon –d –u gpfs，启用默认配额！ 2．设置特殊用户的磁盘配额：(wade 为用户名) a)mmedquota –u wade b)在出现的界面中修改相应的容量限制数值，如下所示： c)修改完毕后 wq 保存退出！ 3 查看 quota 设置 mmrepquota –u –v –a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:2:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#设置磁盘配额"},{"categories":["FS"],"content":" GPFS Quota Reference: Managing GPFS quotas Filesets GPFS fileset level quota management in pureScale environment ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:3:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#gpfs-quota"},{"categories":["FS"],"content":" CLI # 文件集相关信息 mmlsfileset gpfs -L # 启用配额 mmchfs gpfs -Q yes --perfileset-quota # 启用默认配额 mmdefquotaon # 为新用户，组和文件集指定默认配额值 mmdefedquota # 设置默认配额 mmsetquota –F /tmp/defaultQuotaExample # 显式建立或更改文件系统配额限制 #https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_mmedquota.htm mmedquota # 检查配额 mmcheckquota mmcrfileset mmdelfileset mmlinkfileset mmunlinkfileset mmlsfileset gpfs mmdefquotaon -d gpfs mmcrfileset gpfs fset1 -t \"test file set\" mmedquota -j gpfs:fset1 mmcheckquota -v gpfs mmrepquota -g -v -a # mmedquota -t -j # 链接文件集与文件夹 mmlinkfileset gpfs fset1 -J /gpfs/fset1 mmunlinkfileset gpfs fset1 mmcrfileset gpfs fset2 -t \"test file set\" mmedquota -u sunyh mmedquota -u gpfs:fset2:sunyh mmrepquota -v -a mmrepquota -j -v -a ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:3:1","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#cli"},{"categories":["FS"],"content":" REST API Scale management API v2 install \u0026\u0026 manager user https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_configapi.htm#bl1adm_configapi # 默认拥有所有权限的用户名密码 admin:admin001 # 修改admin密码 /usr/lpp/mmfs/gui/cli/chuser admin -p newPassword # 新建管理user usr/lpp/mmfs/gui/cli/mkuser yum install gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm yum install gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm yum install gpfs.java-4.2.3-22.x86_64.rpm yum install gpfs.gui-4.2.3-22.noarch.rpm # yum reinstall gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm # yum reinstall gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm # yum reinstall gpfs.java-4.2.3-22.x86_64.rpm # yum reinstall gpfs.gui-4.2.3-22.noarch.rpm systemctl start gpfsgui systemctl enable gpfsgui cd /usr/lpp/mmfs/gui/cli/ ./mkuser admin -g SecurityAdmin systemctl start gpfsgui systemctl status gpfsgui.service ./mkuser admin -g SecurityAdmin ./mkuser admin001 -g SecurityAdmin curl -k -u admin:admin001 -XGET -H content-type:application/json \"https://192.168.1.50:443/scalemgmt/v2/info\" test curl -k -u admin001:deepbay2010 -XGET -H content-type:application/json \"https://192.168.1.50:443/scalemgmt/v2/info\" Enabling performance tools in management GUI mmperfmon config generate --collectors=gpfs-io-1,gpfs-io-2 mmchnode --perfmon -N gpfs-io-1,gpfs-io-2,gpfs-cli-1,gpfs-cli-2 mmperfmon config update GPFSDiskCap.restrict=gpfs-io-1,gpfs-io-2 GPFSDiskCap.period=86400 mmperfmon config update GPFSFilesetQuota.restrict=gpfs-io-1,gpfs-io-2 GPFSFilesetQuota.period=3600 curl -k -u admin:admin001 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems/gpfs/filesets?fields=:all:' curl -k -u admin001:deepbay2010 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems' curl -k -u admin:admin001 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/config?fields=:all:' scp gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm root@192.168.1.52:/root/rhel72/4.2.3.22/gpfs.gss.pmcollector-4.2.3-22.el7.x86_64.rpm scp gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm root@192.168.1.52:/root/rhel72/4.2.3.22/gpfs.gss.pmsensors-4.2.3-22.el7.x86_64.rpm curl -k -u admin-storage:123456 -X GET --header 'accept:application/json' 'https://192.168.1.50:443/scalemgmt/v2/filesystems' ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:3:2","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#rest-api"},{"categories":["FS"],"content":" TroubleShooting kernel extension does not exist reference: https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1ins_bldgpl.htm#bldgpl /usr/lpp/mmfs/bin/mmbuildgpl --build-package # Wrote: /tmp/deb/gpfs.gplbin-4.15.0-118-generic_4.2.3-22_amd64.deb dpkg -i /tmp/deb/gpfs.gplbin-4.15.0-118-generic_4.2.3-22_amd64.deb ","date":"2019-11-20","objectID":"/posts/gpfs_basics/:4:0","series":null,"tags":["GPFS"],"title":"GPFS安装配置","uri":"/posts/gpfs_basics/#troubleshooting"},{"categories":["node"],"content":"nvm,npm,nrm ","date":"2019-11-20","objectID":"/posts/node_install/:0:0","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#"},{"categories":["node"],"content":" nvmnvm git curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bash source .bashrc nvm --version nvm ls-remote nvm install 10.16.0 which node nvm list nvm use 10 nvm alias default 10 nvm reinstall-packages 10 ","date":"2019-11-20","objectID":"/posts/node_install/:0:1","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#nvm"},{"categories":["node"],"content":" npm npm -v npm install -g npm // -P -D -g ","date":"2019-11-20","objectID":"/posts/node_install/:0:2","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#npm"},{"categories":["node"],"content":" nrm npm install --global nrm nrm test nrm ls nrm use cnpm // 推荐使用cnpm部署私有源镜像服务器 nrm add yourcompany http://registry.npm.yourcompany.com/ ","date":"2019-11-20","objectID":"/posts/node_install/:0:3","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#nrm"},{"categories":["node"],"content":" 源码编译node sudo apt-get install g++ curl libssl-dev apache2-utils git-core build-essential git clone https://github.com/nodejs/node.git cd node ./configure make sudo make install ","date":"2019-11-20","objectID":"/posts/node_install/:0:4","series":null,"tags":["node"],"title":"node安装配置","uri":"/posts/node_install/#源码编译node"},{"categories":["DB"],"content":" redis redis-server stop redis-server start redis-server restart src/redis-server redis.conf bin/mongod -f mongodb.conf ","date":"2019-11-20","objectID":"/posts/redis-cli/:1:0","series":null,"tags":["redis"],"title":"redis cli","uri":"/posts/redis-cli/#redis"},{"categories":["DB"],"content":" redis-cli redis-cli -h \u003chost\u003e -p \u003cport\u003e select 1 //checkout db1, normal db0 keys * type \u003ckey_name\u003e flushdb flushall list LPUSH mylist \"helllo\" LRANGE mylist 0 -1 LLEN mylist set SADD myset \"hello\" SMEMBERS myset SCARD myset zset ZADD myzset 1 \"one\" 2 \"two\" ZRANGE myzset 0 -1 WITHSCORES ZCARD myzset ","date":"2019-11-20","objectID":"/posts/redis-cli/:1:1","series":null,"tags":["redis"],"title":"redis cli","uri":"/posts/redis-cli/#redis-cli"},{"categories":["DB"],"content":" python redis https://github.com/andymccurdy/redis-py/tree/master/tests 1、连接操作相关的命令 quit：关闭连接（connection） auth：简单密码认证 2、对value操作的命令 exists(key)： 确认一个key是否存在 del(key)： 删除一个key type(key)： 返回值的类型 keys(pattern)：返回满足给定pattern的所有key randomkey： 随机返回key空间的一个key rename(oldname, newname)：将key由oldname重命名为newname，若newname存在则删除newname表示的key dbsize： 返回当前数据库中key的数目 expire： 设定一个key的活动时间（s） ttl： 获得一个key的活动时间 select(index)： 按索引查询 move(key, dbindex)：将当前数据库中的key转移到有dbindex索引的数据库 flushdb： 删除当前选择数据库中的所有key flushall： 删除所有数据库中的所有key 3、对String操作的命令 set(key, value)： 给数据库中名称为key的string赋予值value get(key)： 返回数据库中名称为key的string的value getset(key, value)：给名称为key的string赋予上一次的value mget(key1, key2,…, key N)： 返回库中多个string（它们的名称为key1，key2…）的value setnx(key, value)： 如果不存在名称为key的string，则向库中添加string，名称为key，值为value setex(key, time, value)： 向库中添加string（名称为key，值为value）同时，设定过期时间time mset(key1, value1, key2, value2,…key N, value N)： 同时给多个string赋值，名称为key i的string赋值value i msetnx(key1, value1, key2, value2,…key N, value N)：如果所有名称为key i的string都不存在，则向库中添加string，名称key i赋值为value i incr(key)：名称为key的string增1操作 incrby(key, integer)：名称为key的string增加integer decr(key)：名称为key的string减1操作 decrby(key, integer)：名称为key的string减少integer append(key, value)：名称为key的string的值附加value substr(key, start, end)：返回名称为key的string的value的子串 4、对List操作的命令 rpush(key, value)：在名称为key的list尾添加一个值为value的元素 lpush(key, value)：在名称为key的list头添加一个值为value的 元素 llen(key)：返回名称为key的list的长度 lrange(key, start, end)：返回名称为key的list中start至end之间的元素（下标从0开始，下同） ltrim(key, start, end)：截取名称为key的list，保留start至end之间的元素 lindex(key, index)：返回名称为key的list中index位置的元素 lset(key, index, value)：给名称为key的list中index位置的元素赋值为value lrem(key, count, value)：删除count个名称为key的list中值为value的元素。 count为0，删除所有值为value的元素，count\u003e0从头至尾删除count个值为value的元素，count\u003c0从尾到头删除|count|个值为value的元素。 lpop(key)：返回并删除名称为key的list中的首元素 rpop(key)：返回并删除名称为key的list中的尾元素 blpop(key1, key2,… key N, timeout)：lpop命令的block版本。 即当timeout为0时，若遇到名称为key i的list不存在或该list为空，则命令结束。 如果timeout\u003e0，则遇到上述情况时，等待timeout秒，如果问题没有解决，则对keyi+1开始的list执行pop操作。 brpop(key1, key2,… key N, timeout)：rpop的block版本。参考上一命令。 rpoplpush(srckey, dstkey)：返回并删除名称为srckey的list的尾元素，并将该元素添加到名称为dstkey的list的头部 5、对Set操作的命令 sadd(key, member)：向名称为key的set中添加元素member srem(key, member) ：删除名称为key的set中的元素member spop(key) ：随机返回并删除名称为key的set中一个元素 smove(srckey, dstkey, member) ：将member元素从名称为srckey的集合移到名称为dstkey的集合 scard(key) ：返回名称为key的set的基数 sismember(key, member) ：测试member是否是名称为key的set的元素 sinter(key1, key2,…key N) ：求交集 sinterstore(dstkey, key1, key2,…key N) ：求交集并将交集保存到dstkey的集合 sunion(key1, key2,…key N) ：求并集 sunionstore(dstkey, key1, key2,…key N) ：求并集并将并集保存到dstkey的集合 sdiff(key1, key2,…key N) ：求差集 sdiffstore(dstkey, key1, key2,…key N) ：求差集并将差集保存到dstkey的集合 smembers(key) ：返回名称为key的set的所有元素 srandmember(key) ：随机返回名称为key的set的一个元素 6、对zset（sorted set）操作的命令 zadd(key, score, member)：向名称为key的zset中添加元素member，score用于排序。如该元素已存在，则根据score更新该元素的顺序。 zrem(key, member) ：删除名称为key的zset中的元素member zincrby(key, increment, member) ：如果在名称为key的zset中已经存在元素member，则该元素的score增加increment； 否则向集合中添加该元素，其score的值为increment zrank(key, member) ：返回名称为key的zset（元素已按score从小到大排序）中member元素的rank（即index，从0开始）， 若没有member元素，返回“nil” zrevrank(key, member) ：返回名称为key的zset（元素已按score从大到小排序）中member元素的rank（即index，从0开始）， 若没有member元素，返回“nil” zrange(key, start, end)：返回名称为key的zset（元素已按score从小到大排序）中的index从start到end的所有元素 zrevrange(key, start, end)：返回名称为key的zset（元素已按score从大到小排序）中的index从start到end的所有元素 zrangebyscore(key, min, max)：返回名称为key的zset中score \u003e= min且score \u003c= max的所有元素 zcard(key)：返回名称为key的zset的基数 zscore(key, element)：返回名称为key的zset中元素element的score zremrangebyrank(key, min, max)：删除名称为key的zset中rank \u003e= min且rank \u003c= max的所有元素 zremrangebyscore(key, min, max) ：删除名称为key的zset中score \u003e= min且score \u003c= max的所有元素 zunionstore / zinterstore(dstkeyN, key1,…,keyN, WEIGHTS w1,…wN, AGGREGATE SUM|MIN|MAX)：对N个zset求并集和交集， 并将最后的集合保存在dstkeyN中。对于集合中每一个元素的score，在进行AGGREGATE运算前，都要乘以对于的WEIGHT参数。 如果没有提供WEIGHT，默认为1。默认的AGGREGATE是SUM，即结果集合中元素的score是所有集合对应元素进行SUM运算的值， 而MIN和MAX是指，结果集合中元素的score是所有集合对应元素中最小值和最大值。 7、对Hash操作的命令 hset(key, field, value)","date":"2019-11-20","objectID":"/posts/redis-cli/:1:2","series":null,"tags":["redis"],"title":"redis cli","uri":"/posts/redis-cli/#python-redis"},{"categories":null,"content":" 本文由 简悦 SimpRead 转码， 原文地址 aimuch.com ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#"},{"categories":null,"content":" 配置 git 环境","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#配置git环境-配置git环境配置-git-环境"},{"categories":null,"content":" 添加配置1 2 git config [--local | --global | --system] user.name 'Your name' git config [--local | --global | --system] user.email 'Your email' 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:1:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#添加配置-添加配置添加配置"},{"categories":null,"content":" 查看配置1 git config --list [--local | --global | --system] 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:2:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#查看配置-查看配置查看配置"},{"categories":null,"content":" 区别1 2 3 local：区域为本仓库 global: 当前用户的所有仓库 system: 本系统的所有用户 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:3:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#区别-区别区别"},{"categories":null,"content":" git add . 和 git add -u 区别1 2 3 4 5 6 7 8 9 10 11 12 git add . ：将工作空间新增和被修改的文件添加的暂存区 git add -u :将工作空间被修改和被删除的文件添加到暂存区(不包含没有纳入Git管理的新增文件) ``` # 创建仓库 ```bash git init [project folder name] 初始化 git 仓库 git add [fileName] 把文件从工作目录添加到暂存区 git commit -m'some information' 用于提交暂存区的文件 git commit -am'Some information' 用于提交跟踪过的文件 git log 查看历史 git status 查看状态 复制 额外 git add -u 可以添加所有已经被 git 控制的文件到暂存区 以前删除文件夹只会用 「-rf」，今天学到了 「-r」，并得知它们两个区别：「-r」 有时候会提示是否确认删除。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#git-add-和-git-add-u区别-git-add--和-git-add--u区别git-add--和-git-add--u-区别"},{"categories":null,"content":" clone 时指定文件夹名字1 git clone https://github.com/repository_NAME PATH/new_folder 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#clone时指定文件夹名字-clone时指定文件夹名字clone-时指定文件夹名字"},{"categories":null,"content":" 给文件重命名的简便方法1 2 git mv [old file name] [new file name] git commit -m 'some information' 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#给文件重命名的简便方法-给文件重命名的简便方法给文件重命名的简便方法"},{"categories":null,"content":" Tag 标签显示已有标签 1 git tag 复制 新建标签 创建一个含附注类型的标签非常简单，用 -a （译注：取 annotated 的首字母）指定标签名字即可 1 git tag -a tag_name -m 'Some Messages' 复制 删除标签 删除本地标签: 1 git tag -d tag_name 复制 删除 remote 标签 : 1 git push --delete origin tag_name 复制 推送标签到 github 将本地所有标签推送到 remote: 1 git push origin --tags 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#tag标签-tag标签tag-标签"},{"categories":null,"content":" 在 Github 上面创建 Release 在当前 repository 下点击 release 标签: github release 点击 Draft a new release 按钮: github release 在跳转后的界面下填写 Tag version 、 Release title 和 描述 github release 如下图: [ github release ](/2019/03/21/git教程/github-release4.png “github release”) 点击 Update release 按钮提交即可，提交后效果: github release ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#在github上面创建release-在github上面创建release在-github-上面创建-release"},{"categories":null,"content":" 通过 git log 查看版本演变历史1 2 3 4 5 6 git log --all 查看所有分支的历史 git log --all --graph 查看图形化的 log 地址 git log --oneline 查看单行的简洁历史。 git log --oneline -n4 查看最近的4条简洁历史。 git log --oneline --all -n4 --graph 查看所有分支最近4条单行的图形化历史。 git help --web log 跳转到git log 的帮助文档网页 复制1 git branch -v 查看本地有多少分支 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#通过git-log查看版本演变历史-通过git-log查看版本演变历史通过-git-log-查看版本演变历史"},{"categories":null,"content":" 通过图形界面工具来查看版本历史1 gitk 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#通过图形界面工具来查看版本历史-通过图形界面工具来查看版本历史通过图形界面工具来查看版本历史"},{"categories":null,"content":" 探密.git 目录查看.git 文件夹下的内容： 1 ls .git/ -al 复制 如下: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 drwxr-xr-x 1 Andy 197609 0 12月 17 22:38 ./ drwxr-xr-x 1 Andy 197609 0 12月 17 21:50 ../ -rw-r--r-- 1 Andy 197609 7 12月 17 22:38 COMMIT_EDITMSG -rw-r--r-- 1 Andy 197609 301 12月 12 22:55 config -rw-r--r-- 1 Andy 197609 73 12月 12 22:55 description -rw-r--r-- 1 Andy 197609 96 12月 19 00:00 FETCH_HEAD -rw-r--r-- 1 Andy 197609 23 12月 12 22:55 HEAD drwxr-xr-x 1 Andy 197609 0 12月 12 22:55 hooks/ -rw-r--r-- 1 Andy 197609 249 12月 17 22:38 index drwxr-xr-x 1 Andy 197609 0 12月 12 22:55 info/ drwxr-xr-x 1 Andy 197609 0 12月 12 22:55 logs/ drwxr-xr-x 1 Andy 197609 0 12月 17 22:38 objects/ -rw-r--r-- 1 Andy 197609 114 12月 12 22:55 packed-refs drwxr-xr-x 1 Andy 197609 0 12月 12 22:55 refs/ 复制1 2 3 4 5 6 cat命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。 cat HEAD 查看HEAD文件的内容 git cat-file 命令 显示版本库对象的内容、类型及大小信息。 git cat-file -t b44dd71d62a5a8ed3 显示版本库对象的类型 git cat-file -s b44dd71d62a5a8ed3 显示版本库对象的大小 git cat-file -p b44dd71d62a5a8ed3 显示版本库对象的内容 复制 .git 里几个常用的如下： 1 2 3 4 5 6 HEAD：指向当前的工作路径 config：存放本地仓库（local）相关的配置信息。 refs/heads: 存放分支 refs/heads/master/: 指向master分支最后一次commit refs/tags: 存放tag，又叫里程牌 （当这次commit是具有里程碑意义的 比如项目1.0的时候 就可以打tag） objects：核心文件，存储文件 复制 .git/objects/ 存放所有的 git 对象，对象哈希值前 2 位作为文件夹名称，后 38 位作为对象文件名，可通过 git cat-file -p 命令，拼接文件夹名称 + 文件名查看。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#探密-git目录-探密git目录探密git-目录"},{"categories":null,"content":" commit、tree 和 blob 三个对象之间的关系 tree 1 2 3 commit: 提交时的镜像 tree: 文件夹 blob: 文件 复制 【同学问题】 每次 commit，git 都会将当前项目的所有文件夹及文件快照保存到 objects 目录，如果项目文件比较大，不断迭代，commit 无数次后，objects 目录中文件大小是不是会变得无限大？ 【老师解答】 Git 对于内容相同的文件只会存一个 blob，不同的 commit 的区别是 commit、tree 和有差异的 blob，多数未变更的文件对应的 blob 都是相同的，这么设计对于版本管理系统来说可以省很多存储空间。其次，Git 还有增量存储的机制，我估计是对于差异很小的 blob 设计的吧。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#committree和blob三个对象之间的关系-committree和blob三个对象之间的关系committree-和-blob-三个对象之间的关系"},{"categories":null,"content":" 分离头指针情况下的注意事项detached HEAD ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#分离头指针情况下的注意事项-分离头指针情况下的注意事项分离头指针情况下的注意事项"},{"categories":null,"content":" 进一步理解 HEAD 和 branch1 2 3 4 5 6 7 8 9 10 11 git checkout -b new_branch [具体分支 或 commit] 创建新分支并切换到新分支 git diff HEAD HEAD~1 比较最近两次提交 git diff HEAD HEAD~2 比较最近和倒数第三次提交 git diff HEAD HEAD^ 比较最近两次提交 git diff HEAD HEAD^^ 比较最近和倒数第三次提交 ``` # 怎么删除不需要的分支？ 查看分支： ```bash git branch -av 复制 删除分支命令： 1 2 git branch -d [branch name] #删除 git branch -D [branch name] #强制删除 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#进一步理解head和branch-进一步理解head和branch进一步理解-head-和-branch"},{"categories":null,"content":" 怎么修改最新 commit 的 message1 git commit --amend 对最近一次的commit信息进行修改 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎么修改最新commit的message-怎么修改最新commit的message怎么修改最新-commit-的-message"},{"categories":null,"content":" 怎么修改老旧 commit 的 message1 git rebase -i [要更改的commit的上一级commit] 复制 接下来就是一个交互过程… 这期间会产生一个 detached HEAD，然后将改好的 commit 指向该 detached HEAD，如下图所示： rebase git rebase 工作的过程中，就是用了分离头指针。rebase 意味着基于新 base 的 commit 来变更部分 commits。它处理的时候，把 HEAD 指向 base 的 commit，此时如果该 commit 没有对应 branch，就处于分离头指针的状态，然后重新一个一个生成新的 commit，当 rebase 创建完最后一个 commit 后，结束分离头状态，Git 让变完基的分支名指向 HEAD。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎么修改老旧commit的message-怎么修改老旧commit的message怎么修改老旧-commit-的-message"},{"categories":null,"content":" 怎样把连续的多个 commit 整理成 1 个1 git rebase -i [要更改的commit的上一级commit] 复制1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ git log --graph * commit 7d3386842a2168ae630b65f687364243139c893c (HEAD -\u003e master, origin/master, origin/HEAD) | Author: aimuch \u003canetspace@gmail.com\u003e | Date: Thu Dec 20 23:34:18 2018 +0800 | | update | * commit 9eb3188bbc63cae1bfed5f9dfc1593019e360a6a | Author: aimuch \u003canetspace@gmail.com\u003e | Date: Wed Dec 19 20:30:14 2018 +0800 | | update | * commit bbe6d53e2b477f2d2aa402af7f315ecdfc63459e | Author: aimuch \u003canetspace@gmail.com\u003e | Date: Wed Dec 19 20:12:29 2018 +0800 | | update | * commit 7735d66ded7f98adeca93d96fb7be12ffb67c76a | Author: aimuch \u003canetspace@gmail.com\u003e | Date: Wed Dec 19 00:27:00 2018 +0800 | | update | * commit d9f9d115fab425b5654f8ccfec6a996aef35b76b | Author: aimuch \u003canetspace@gmail.com\u003e | Date: Wed Dec 19 00:23:36 2018 +0800 | | update 复制1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pick 7735d66 update #合并到该commit上 squash bbe6d53 update squash 9eb3188 update squash 7d33868 update # Rebase d9f9d11..7d33868 onto d9f9d11 (4 commands) # # Commands: # p, pick \u003ccommit\u003e = use commit # r, reword \u003ccommit\u003e = use commit, but edit the commit message # e, edit \u003ccommit\u003e = use commit, but stop for amending # s, squash \u003ccommit\u003e = use commit, but meld into previous commit # f, fixup \u003ccommit\u003e = like \"squash\", but discard this commit's log message # x, exec \u003ccommand\u003e = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop \u003ccommit\u003e = remove commit # l, label \u003clabel\u003e = label current HEAD with a name # t, reset \u003clabel\u003e = reset HEAD to a label # m, merge [-C \u003ccommit\u003e | -c \u003ccommit\u003e] \u003clabel\u003e [# \u003coneline\u003e] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c \u003ccommit\u003e to reword the commit message. 复制1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # This is a combination of 4 commits. # This is the 1st commit message: update # This is the commit message #2: update # This is the commit message #3: update # This is the commit message #4: update 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎样把连续的多个commit整理成1个-怎样把连续的多个commit整理成1个怎样把连续的多个-commit-整理成-1-个"},{"categories":null,"content":" 添加忽略配置文件 gitignore在 git 中如果想忽略掉某个文件， 不让这个文件提交到版本库中，可以使用修改 .gitignore 文件的方法。 这个文件每一行保存了一个匹配的规则，可以用正则表达式来描述，例如: 1 2 3 4 5 6 7 8 # 此为注释 – 将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt *ignore/ # 忽略名称中末尾为ignore的文件夹 *ignore*/ # 忽略名称中间包含ignore的文件夹 复制 通用的模板: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Compiled source # ################### *.com *.class *.dll *.exe *.o *.so # Packages # ############ # it's better to unpack these files and commit the raw source # git has its own built in compression methods *.7z *.dmg *.gz *.iso *.jar *.rar *.tar *.zip # Logs and databases # ###################### *.log *.sql *.sqlite # OS generated files # ###################### .DS_Store .DS_Store? ._* .Spotlight-V100 .Trashes Icon? ehthumbs.db Thumbs.db 复制 更详细的介绍请看 GitHub 官网给出的例子: https://github.com/github/gitignore ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#添加忽略配置文件gitignore-添加忽略配置文件gitignore添加忽略配置文件-gitignore"},{"categories":null,"content":" Git 修改 gitignore 后生效1 2 3 4 git rm -r --cached . #清除缓存 git add . #重新trace file git commit -m \"update .gitignore\" #提交和注释 git push origin master #可选，如果需要同步到remote上的话 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#git修改gitignore后生效-git修改gitignore后生效git-修改-gitignore-后生效"},{"categories":null,"content":" 寻找并删除 Git 记录中的大文件本文来介绍查找和重写 Git 记录的命令：git rev-list , git filter-branch 。生产环境请考虑使用 bfg 等效率工具。 首先通过 rev-list 来找到仓库记录中的大文件： 1 git rev-list --objects --all | grep \"$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '{print$1}')\" 复制 然后通过 filter-branch 来重写这些大文件涉及到的所有提交（重写历史记录）： 1 git filter-branch -f --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch your-file-name' --tag-name-filter cat -- --all 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#寻找并删除git记录中的大文件-寻找并删除git记录中的大文件寻找并删除-git-记录中的大文件"},{"categories":null,"content":" Git 仓库的存储方式如果你熟知 Git 的存储方式，跳过此节。 Git 仓库位于项目根目录的 .git 文件夹，其中保存了从仓库建立（git init）以来所有的代码增删。 每一个提交（Commit）相当于一个 Patch 应用在之前的项目上，借此一个项目可以回到任何一次提交时的文件状态。 于是在 Git 中删除一个文件时，Git 只是记录了该删除操作，该记录作为一个 Patch 存储在 .git 中。 删除前的文件仍然在 Git 仓库中保存着。直接删除文件并提交起不到给 Git 仓库瘦身的效果。 在 Git 仓库彻底删除一个文件只有一种办法：重写（Rewrite）涉及该文件的所有提交。 幸运的是借助 git filter-branch 便可以重写历史提交，当然这也是 Git 中最危险的操作。 可以说比 rm -rf * 危险一万倍。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:1:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#git仓库的存储方式-git仓库的存储方式git-仓库的存储方式"},{"categories":null,"content":" 从所有提交中删除一个文件如果清楚地记得曾提交过名为 recent-badge.psd 的文件。这是一个很大的 PhotoShop 文件，要把它删掉。 filter-branch 命令可以用来重写 Git 仓库中的提交， 利用 filter-branch 的 --index-filter 参数便能把它从所有 Git 提交中删除。 1 2 3 4 5 6 $ git filter-branch -f --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch assets/img/recent-badge.psd' --tag-name-filter cat -- --all Rewrite 2771f50d45a0293668a30af77983d87886441640 (264/982)rm 'assets/img/recent-badge.psd' Rewrite 1a98ecb3f39e1f200e31754714eec18bc92848ce (265/982)rm 'assets/img/recent-badge.psd' Rewrite d4e61cfb1d88187b0561d283e663b81b738df2c7 (270/982)rm 'assets/img/recent-badge.psd' Rewrite 4ba0df06b26cf86fd39c2cda6b012c521cbc4dc1 (271/982)rm 'assets/img/recent-badge.psd' Rewrite 242ae98060c77863f5e826ba7e1ec47 复制 filter-branch 是让 git 重写每一个分支； --prune-empty 选项告诉 git，如果因为重写导致某些 commit 变成了空（比如修改的文件全部被删除），那么忽略掉这个 commit; --index-filter 参数用来指定一条 Bash 命令，然后 Git 会检出（checkout）所有的提交， 执行该命令，然后重新提交。我们在提交前移除了 recent-badge.psd 文件， 这个文件便从 Git 的所有记录中完全消失了； --tag-name-filter 表示对每一个 tag 如何重命名，重命名的命令紧跟在后面，当前的 tag 名会从标注输入送给后面的命令，用 cat 就表示保持 tag 名不变； 紧跟着的 -- 表示分割符； --all 参数告诉 Git 我们需要重写所有分支（或引用）。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:2:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#从所有提交中删除一个文件-从所有提交中删除一个文件从所有提交中删除一个文件"},{"categories":null,"content":" 寻找大文件的 ID删掉了 recent-badge.psd 后我仍不满足，我要找到所有的大文件，并把它删掉。 verify-pack 命令用来验证 Git 打包的归档文件，我们用它来找到那些大文件。 例如： 1 2 3 4 5 6 7 $ git verify-pack -v .git/objects/pack/*.idx 8fa15d279de33ce28a3289fd33951374084231e4 tree 135 137 144088922 a44a50b2ffb1f8283c8e64aafb8e7628249d7453 tree 33 43 144089059 b57d99f38fe22491e4a2d30c2b081ecb7bbb329c tree 99 97 144089102 2d4ffaffc11758d561ea1a6d57dd8ee17ee1d836 blob 644952 644959 144089199 8cf81ebfeec409f19e7a47a76517317f3bfa268d blob 695898 695871 144734158 ... 复制 -v（verbose）参数是打印详细信息。 输出的第一列是文件 ID，第二列表示文件（blob）或目录（tree），第三列是文件大小。 现在得到了所有的文件 ID 及其大小，需要写一点 Bash 了！ 先按照第三列排序，并取最大的 5 条，然后打印出每项的第一列（这一列是文件 ID）： 1 2 3 4 5 6 $ git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -10 | awk '{print$1}')\" f846f156d16f74243b67e3dabec58a3128744352 4a1546e732b2e2a352b7bf220c1a22ad859abf89 f72d04efe6d0b41b067f9fbbc62455f28d3670d2 49bdf300ddf57d1946bc9c6570d94a38ac9d6a50 9c073d4177af5d2e43ada41f92efb18d9462a536 复制 现在变得到了最大的 5 个文件的 ID，而我需要文件名才能用 filter-branch 移除它。 我现在需要文件 ID 和文件名的映射关系。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:3:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#寻找大文件的id-寻找大文件的id寻找大文件的-id"},{"categories":null,"content":" 文件名与 ID 映射rev-list 命令用来列出 Git 仓库中的提交，我们用它来列出所有提交中涉及的文件名及其 ID。 该命令可以指定只显示某个引用（或分支）的上下游的提交。例如： 1 git rev-list foo bar ^baz 复制 将会列出所有从 foo 和 bar 可到达，但从 baz 不可到达的提交。我们将会用到 rev-list 的两个参数： --objects：列出该提交涉及的所有文件 ID。 --all：所有分支的提交，相当于指定了位于 /refs 下的所有引用。 我们看看这条命令的输出： 1 2 3 4 5 6 7 8 9 $ git rev-list --objects --all c252878ac09a3979a80520b82a71dc2dae4529f9 7bc7d05c6097063f531580ba4c32921464a6c456 _drafts dcce26ed53fbb869dc7d5b71742d2f9e523bfe42 _layouts 414186c794a0d58695abb75c548bdbfec1de2763 _layouts/default.html 1934eeffe3d242375510dff28cffa6de6b3de367 _layouts/post.html 5f14647875f2177a6d37b8bfbcdb4629af595b64 _posts 6cdbb293d453ced07e6a07e0aa6e580e6a5538f4 _posts/2013-10-12-2.md ... 复制 现在就得到了文件名（如_posts/2013-10-12-2.md）和 ID（如 6cdbb293d453ced07e6a07e0aa6e580e6a5538f4 ）的映射关系。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:4:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#文件名与id映射-文件名与id映射文件名与-id-映射"},{"categories":null,"content":" 得到文件名列表前面我们通过 rev-list 得到了文件名 - ID 的对应关系，通过 verify-pack 得到了最大的 5 个文件 ID。 用后者筛选前者便能得到最大的 5 个文件的文件名： 1 2 3 4 5 6 7 $ git rev-list --objects --all | grep \"$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '{print$1}')\" #$ git rev-list --objects --all | grep \"$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '{print$1}')\" \u003e large-files.txt f846f156d16f74243b67e3dabec58a3128744352 assets/img/recent-badge.psd 4a1546e732b2e2a352b7bf220c1a22ad859abf89 assets/img/album/me/IMG_0276.JPG f72d04efe6d0b41b067f9fbbc62455f28d3670d2 assets/img/album/me/IMG_0389.JPG 49bdf300ddf57d1946bc9c6570d94a38ac9d6a50 assets/img/album/me/IMG_0813.JPG 9c073d4177af5d2e43ada41f92efb18d9462a536 assets/img/album/me/IMG_0891.JPG 复制 先把上面输出存到 large-files.txt 中。还记得吗？--tree-filter 参数中我们需要给出一行的文件名列表。上述列表我们需要处理一下： 1 2 $ cat large-files.txt| awk '{print $2}' | tr '\\n' ' ' assets/img/recent-badge.psd assets/img/album/me/IMG_0276.JPG assets/img/album/me/IMG_0389.JPG assets/img/album/me/IMG_0813.JPG assets/img/album/me/IMG_0891.JPG 复制 现在便得到了一行的文件列表。把它存到 large-files-inline.txt 中。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:5:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#得到文件名列表-得到文件名列表得到文件名列表"},{"categories":null,"content":" 删除所有大文件现在得到了要删除的大文件列表 large-files-inline.txt，把它传入到 --tree-filter 中即可： 1 git filter-branch -f --prune-empty --index-filter \"git rm -rf --cached --ignore-unmatch `cat large-files-inline.txt`\" --tag-name-filter cat -- --all 复制 注意这里 --index-filter 的参数要用双引号，因为 cat large-files-inline.txt 还需要 Bash 的解析。 至此已经干掉了那些大文件，来看看瘦身了多少吧！ 注意 filter-branch 之后.git 目录下会有大量的备份。 当然到此为止我们更改的都是本地仓库，现在把这些改变 Push 到远程仓库中去！ 1 git push origin --force --all 复制 因为不是 fast forward，所以需要指定 --force 参数。 这里的 --all 会将所有分支都推送到 origin 上。当然你也可以只推送 master 分支：git push origin master --force。但是！如果其它远程分支有那些大文件提交的话，仍然没有瘦身！ ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:6:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#删除所有大文件-删除所有大文件删除所有大文件"},{"categories":null,"content":" 怎么比较暂存区和 HEAD 所含文件的差异？1 git diff --cached 复制 或者 1 git diff --staged 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎么比较暂存区和head所含文件的差异-怎么比较暂存区和head所含文件的差异怎么比较暂存区和-head-所含文件的差异"},{"categories":null,"content":" 怎么比较工作区和暂存区所含文件的差异？1 git diff 复制1 git diff -- [filename/pathname] #比较具体的文件或者路径 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎么比较工作区和暂存区所含文件的差异-怎么比较工作区和暂存区所含文件的差异怎么比较工作区和暂存区所含文件的差异"},{"categories":null,"content":" 如何让暂存区恢复成和 HEAD 的一样？1 git reset HEAD 复制1 2 3 4 git reset 有三个参数 --soft 这个只是把 HEAD 指向的 commit 恢复到你指定的 commit，暂存区 工作区不变 --hard 这个是 把 HEAD， 暂存区， 工作区 都修改为 你指定的 commit 的时候的文件状态 --mixed 这个是不加时候的默认参数，把 HEAD，暂存区 修改为 你指定的 commit 的时候的文件状态，工作区保持不变 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#如何让暂存区恢复成和head的一样-如何让暂存区恢复成和head的一样如何让暂存区恢复成和-head-的一样"},{"categories":null,"content":" 如何让工作区的文件恢复为和暂存区一样？1 git checkout -- \u003cfile\u003e... 复制 恢复工作区用 checkout，恢复暂存区用 reset。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#如何让工作区的文件恢复为和暂存区一样-如何让工作区的文件恢复为和暂存区一样如何让工作区的文件恢复为和暂存区一样"},{"categories":null,"content":" 怎样取消暂存区部分文件的更改？1 git reset HEAD -- \u003cfile\u003e... 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎样取消暂存区部分文件的更改-怎样取消暂存区部分文件的更改怎样取消暂存区部分文件的更改"},{"categories":null,"content":" 看看不同提交的指定文件的差异1 git diff commit-id1 commit-id2 -- \u003cfile\u003e... 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#看看不同提交的指定文件的差异-看看不同提交的指定文件的差异看看不同提交的指定文件的差异"},{"categories":null,"content":" 正确删除文件的方法1 git rm \u003cfile\u003e 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#正确删除文件的方法-正确删除文件的方法正确删除文件的方法"},{"categories":null,"content":" 开发中临时加塞了紧急任务怎么处理？1 2 git stash list #查看stash中存放的信息 git stash #将当前工作区内容存放到\"堆栈\"中 复制1 git stash apply #把\"堆栈\"里面的内容弹出到工作区中，同时\"堆栈\"中信息还在 复制1 git stash pop #把\"堆栈\"里面的内容弹出到工作区中，同时丢弃\"堆栈\"中最新的信息 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#开发中临时加塞了紧急任务怎么处理-开发中临时加塞了紧急任务怎么处理开发中临时加塞了紧急任务怎么处理"},{"categories":null,"content":" 如何指定不需要 Git 管理的文件？1 .gitignore 复制 【同学提问】 如果提交 commit 后，想再忽略一些已经提交的文件，怎么处理。 【老师回答】 The problem is that .gitignore ignores just files that weren’t tracked before (by git add). Run git reset name_of_file to unstage the file and keep it. In case you want to also remove given file from the repository (after pushing), use git rm –cached name_of_file. 把想忽略的文件添加到 .gitignore ；然后通过 git rm – cached name_of_file 的方式删除掉 git 仓库里面无需跟踪的文件。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#如何指定不需要git管理的文件-如何指定不需要git管理的文件如何指定不需要-git-管理的文件"},{"categories":null,"content":" 添加远程仓库1 git remote add [shortname] [url] 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#添加远程仓库-添加远程仓库添加远程仓库"},{"categories":null,"content":" 配置公私钥","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#配置公私钥-配置公私钥配置公私钥"},{"categories":null,"content":" 检查是否已存在相应的 ssh key:打开终端，输入: 1 ls -al ~/.ssh 复制 核对列出来的 ssh key 是否有已存在的，假如你没有看到列出的公私钥对，或是不想再用之前的公私钥对，你可以选择下面的步骤生成新的公私钥对. ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:1:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#检查是否已存在相应的ssh-key-检查是否已存在相应的ssh-key检查是否已存在相应的-ssh-key"},{"categories":null,"content":" 生成新的 ssh key, 并添加至 ssh-agent:","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:2:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#生成新的ssh-key-并添加至ssh-agent-生成新的ssh-key并添加至ssh-agent生成新的-ssh-key-并添加至-ssh-agent"},{"categories":null,"content":" 打开终端，使用 ssh key 生成命令：1 ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" 复制 注意 ：后面的邮箱对应相应账号的邮箱，假如是 github 的账号，且注册账号的邮箱为 xxx@gmail.com，则命令行为： 1 2 3 4 5 6 7 ssh-keygen -t rsa -b 4096 -C \"xxx@gmail.com\"`。 ``` ### 接下来会提示你保存的`ssh key`的名称以及路径。 默认路径是`/home/you/.ssh/id_rsa`(`you`为用户个人目录)即`~/.ssh/id_rsa`。这一步很重要，如果你使用默认的，且下一个账号也是使用默认的路径和文件名，那么之前的`ssh key`就会被后来生成的`ssh key`重写，从而导致之前的账号不可用。因此，正确的做法是给它命名，最后以应用名进行命名，因为更容易区分。以下是我个人配的： ```shell /home/andy/.ssh/github_rsa 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:2:1","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#打开终端-使用ssh-key生成命令-打开终端-使用ssh-key生成命令打开终端使用-ssh-key-生成命令"},{"categories":null,"content":" 接下来会提示设置 ssh 安全密码。这一步可以使用默认的（即不设置密码），直接按回车即可。 这里会生成 xxx_rsa 和 xxx_rsa.pub 两个文件，xxx_rsa 是生成的 ssh key 的私钥名，xxx_rsa.pub 是生成的 ssh key 的公钥名，私钥要放在本地，公钥要放在服务器或 github 的 Settings-\u003eSSHand GPG keys-\u003eNew SSH key 上。 git key ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:2:2","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#接下来会提示设置ssh安全密码-接下来会提示设置ssh安全密码接下来会提示设置-ssh-安全密码"},{"categories":null,"content":" ssh key 生成后，接下来需要为 ssh key 添加代理。这是为了让请求自动对应相应的账号。网上很多文章写到需要另外配置 config 文件，经本人亲测，其实是不需要的，在生成了 ssh key 后，通过为生成的 ssh key 添加代理即可，为 ssh key 添加代理命令:ssh-add ~/.ssh/xxx_rsa， xxx_rsa 是你生成的 ssh key 的私钥名，我的设置为: 1 ssh-add ~/.ssh/github_rsa 复制 这里有可能会提示以下错误: ssh-add key error 解决方法: 需要 ssh-agent 启动 bash，或者说把 bash 挂到 ssh-agent 下面 1 ssh-agent bash --login -i 复制 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:2:3","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#ssh-key生成后接下来需要为ssh-key添加代理-ssh-key生成后接下来需要为ssh-key添加代理ssh-key-生成后接下来需要为-ssh-key-添加代理"},{"categories":null,"content":" 将生成的 xxx_rsa.pub 公钥内容添加到 GitHub 的 SSH keys 页面上。 github ssh ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:3:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#将生成的xxx-rsa-pub公钥内容添加到github的ssh-keys页面上-将生成的xxx_rsapub公钥内容添加到github的ssh-keys页面上将生成的-xxx_rsapub-公钥内容添加到-github-的-ssh-keys-页面上"},{"categories":null,"content":" 连接测试接下来我们测试是否配置成功，打开终端，输入: 1 ssh -T git@github.com 复制 git test ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:4:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#连接测试-连接测试连接测试"},{"categories":null,"content":" 怎么快速淘到感兴趣的开源项目UI 界面高级搜索： https://github.com/search/advanced 命令高级搜索： 1 git 最好 学习 资料 in:readme stars:\u003e1000 language:c 复制 上述命令的意思是搜索 reademe 中包含 git、最好、学习、资料” 且 star大于1000 的，用 C语言编写的仓库。 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#怎么快速淘到感兴趣的开源项目-怎么快速淘到感兴趣的开源项目怎么快速淘到感兴趣的开源项目"},{"categories":null,"content":" 参考资料 git-cheat-sheet 寻找并删除 Git 记录中的大文件 GitHub 官网给出的例子 ","date":"0001-01-01","objectID":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/:0:0","series":null,"tags":null,"title":"","uri":"/posts/collections/git-%E6%95%99%E7%A8%8B-aimuch/#参考资料-参考资料参考资料"}]